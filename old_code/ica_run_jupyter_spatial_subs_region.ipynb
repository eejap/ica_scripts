{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f393788d-f840-4df8-ac9e-eae86f0626ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from scipy.interpolate import interp2d\n",
    "from numpy.ma import masked_array\n",
    "import glob\n",
    "import h5py\n",
    "from datetime import datetime, timedelta\n",
    "import geopandas as gpd\n",
    "from scipy.stats import linregress\n",
    "from matplotlib.dates import date2num, num2date\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from rasterio.transform import from_origin\n",
    "import netCDF4 as nc\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import OptimizeWarning\n",
    "from tqdm import tqdm\n",
    "\n",
    "# this is Andrew Watson's library of functions, see https://github.com/Active-Tectonics-Leeds/interseismic_practical\n",
    "import sys\n",
    "import interseis_lib as lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1360d9a5-5917-4e12-821e-1d226b304f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calc_model function\n",
    "def calc_model(dph, imdates_ordinal, model):\n",
    "    imdates_years = imdates_ordinal / 365.25  # Convert ordinal dates to years\n",
    "\n",
    "    # Construct design matrix A based on the selected model\n",
    "    A = sm.add_constant(imdates_years)  # Add constant term\n",
    "\n",
    "    if model == 1:  # Annual+L\n",
    "        sin = np.sin(2 * np.pi * imdates_years)\n",
    "        cos = np.cos(2 * np.pi * imdates_years)\n",
    "        A = np.concatenate((A, sin[:, np.newaxis], cos[:, np.newaxis]), axis=1)\n",
    "    elif model == 2:  # Quad\n",
    "        A = np.concatenate((A, (imdates_years ** 2)[:, np.newaxis]), axis=1)\n",
    "    elif model == 3:  # Annual+Q\n",
    "        sin = np.sin(2 * np.pi * imdates_years)\n",
    "        cos = np.cos(2 * np.pi * imdates_years)\n",
    "        A = np.concatenate((A, (imdates_years ** 2)[:, np.newaxis], sin[:, np.newaxis], cos[:, np.newaxis]),\n",
    "                           axis=1)\n",
    "\n",
    "    # Fit OLS model and predict values\n",
    "    result = sm.OLS(dph, A, missing='drop').fit()\n",
    "    yvalues = result.predict(A)\n",
    "\n",
    "    return yvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cf154e-bd5d-4d2a-bb44-3498c20b68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files for frames\n",
    "\n",
    "region_names = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/ne_iran_data/region_names.txt\"\n",
    "frames= \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/ne_iran_data/frames.txt\"\n",
    "cumh5_dir = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/ne_iran_data/cumh5\"\n",
    "mask_dir = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/ne_iran_data/mask\"\n",
    "EQA_dir = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/ne_iran_data/EQA.dem_par\"\n",
    "subs_poly_path = '/home/users/eejap002/ica_scripts/polygons/vU_merge_161023_noBabRas_WGS84_fillednosmooth_wmean_rad2_dist18_ltemin10_polygonised_dissolved.shp'\n",
    "\n",
    "with open(frames, \"r\") as file:\n",
    "    frames_list = file.read().splitlines()\n",
    "\n",
    "frames_data = []\n",
    "\n",
    "for frame in frames_list: \n",
    "    EQA_par_pattern = os.path.join(EQA_dir,f\"*{frame}_GEOCml*GACOSmask_EQA.dem_par\")\n",
    "    EQA_par_file = glob.glob(EQA_par_pattern)\n",
    "    cumh5_pattern = os.path.join(cumh5_dir,f\"*{frame}_GEOCml*GACOSmask_cum.h5\")\n",
    "    cumh5_file = glob.glob(cumh5_pattern)\n",
    "    mask_pattern = os.path.join(mask_dir,f\"*{frame}_GEOCml*GACOSmask_coh_03_mask.geo.tif\")\n",
    "    mask_file = glob.glob(mask_pattern)\n",
    "\n",
    "    with h5py.File(cumh5_file[0], 'r') as file:\n",
    "        imdates = file['imdates']\n",
    "        imdates = imdates[:] \n",
    "        vel = file['vel']\n",
    "        vel = vel[:]\n",
    "        cum = file['cum']\n",
    "        cum = cum[:]\n",
    "\n",
    "    dates=[]\n",
    "    for date_value in imdates:\n",
    "        date_string = str(date_value)  # Convert int32 to string\n",
    "        year = int(date_string[:4])\n",
    "        month = int(date_string[4:6])\n",
    "        day = int(date_string[6:])\n",
    "        real_date = datetime(year, month, day)\n",
    "        dates.append(real_date)    \n",
    "\n",
    "    width = int(lib.get_par(EQA_par_file[0],'width'))\n",
    "    length = int(lib.get_par(EQA_par_file[0],'nlines'))\n",
    "    \n",
    "    # get corner positions\n",
    "    corner_lat = float(lib.get_par(EQA_par_file[0], 'corner_lat'))\n",
    "    corner_lon = float(lib.get_par(EQA_par_file[0], 'corner_lon'))\n",
    "\n",
    "    # get post spacing (distance between velocity measurements)\n",
    "    post_lat = float(lib.get_par(EQA_par_file[0],'post_lat'))\n",
    "    post_lon = float(lib.get_par(EQA_par_file[0],'post_lon'))\n",
    "\n",
    "    # calculate grid spacings\n",
    "    lat = corner_lat + post_lat*np.arange(1,length+1) - post_lat/2\n",
    "    lon = corner_lon + post_lon*np.arange(1,width+1) - post_lon/2\n",
    "\n",
    "    frames_data.append({\n",
    "        'frame': frame,\n",
    "        'EQA_file': EQA_par_file,\n",
    "        'cumh5_file': cumh5_file,\n",
    "        'mask_file': mask_file,\n",
    "        'imdates': imdates,\n",
    "        'vel': vel,\n",
    "        'cum': cum,\n",
    "        'dates': dates,\n",
    "        'width': width,\n",
    "        'length': length,\n",
    "        'corner_lat': corner_lat,\n",
    "        'corner_lon': corner_lon,\n",
    "        'post_lat': post_lat,\n",
    "        'post_lon': post_lon,\n",
    "        'lat': lat,\n",
    "        'lon': lon\n",
    "            })\n",
    "\n",
    "    # Create GeoDataFrame\n",
    "    frames_gdf = gpd.GeoDataFrame(frames_data, columns=['frame', 'EQA_file', 'cumh5_file', 'mask_file', 'imdates', 'vel', 'cum', 'dates', 'width', 'length', 'corner_lat', 'corner_lon', 'post_lat', 'post_lon','lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1988e035-57e0-4b92-9cdb-e4e4ebb03bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 1084164)\n",
      "(183, 1086168)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f17b5a374c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cum shape is (t, lat, lon)\n",
    "# we want to make an array of (time, pixels)\n",
    "# we want to reshape cum (202, 268, 327) into (202,(268*327))\n",
    "\n",
    "frames_gdf[\"cum_\"] = \"\"\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_data = row['cum']\n",
    "\n",
    "    # Check if 'cum' data is not empty\n",
    "    if not np.isnan(cum_data).all():\n",
    "        cum_shape = cum_data.shape\n",
    "        n_pix = cum_shape[1] * cum_shape[2]\n",
    "\n",
    "        cum_ = np.reshape(cum_data, (cum_shape[0], n_pix))\n",
    "        print(cum_.shape)\n",
    "        frames_gdf.at[index, 'cum_'] = cum_\n",
    "plt.imshow(cum_[:,40000:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e723a0-17d6-402e-8c0b-aaf0a3cfa5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 1084164)\n",
      "(261, 1084164)\n",
      "(183, 1086168)\n",
      "(183, 1086168)\n"
     ]
    }
   ],
   "source": [
    "# we have cum of shape 202, 268, 327 and mask tif. We want to mask cum with mask tif \n",
    "# i.e. make pixels in cum nan where mask_tif = 0. NaNs are where no data e.g. outside of frame, low coh...\n",
    "\n",
    "frames_gdf[\"cum_masked\"] = \"\"\n",
    "frames_gdf[\"cum_masked_3d\"] = \"\"\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_data = row['cum_']\n",
    "    cum_data_3d = row['cum']\n",
    "    mask = row['mask_file']\n",
    "\n",
    "    with rasterio.open(mask[0]) as tif:\n",
    "    # Read the raster data\n",
    "        mask_tif = tif.read(1)\n",
    "\n",
    "        # Reshape the mask to 1D\n",
    "        mask_1d = mask_tif.flatten()\n",
    "\n",
    "        # Tile the first row along the rows to match the shape of asc_cum_\n",
    "        mask_2d = np.tile(mask_1d, (cum_data.shape[0],1))\n",
    "        print(mask_2d.shape)\n",
    "        \n",
    "        # Apply the mask to every element in the 3D array\n",
    "        cum_masked = cum_data * mask_2d\n",
    "        cum_masked_3d = cum_masked.reshape(cum_data_3d.shape)\n",
    "        \n",
    "        frames_gdf.at[index, 'cum_masked'] = cum_masked\n",
    "        frames_gdf.at[index, 'cum_masked_3d'] = cum_masked_3d\n",
    "        print(cum_masked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f86bef8-0e82-41b8-bf9a-e1680c85ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clip data to subsidence regions\n",
    "# gdf_polygons = gpd.read_file(subs_poly_path)\n",
    "\n",
    "# # keep only subsiding polygon pixels from cum_no_zeros_no_nans\n",
    "# # convert polygons shape into a geopandas dataframe\n",
    "# frames_gdf[\"cum_polygon_masked_3d\"] = \"\"\n",
    "# frames_gdf[\"cum_polygon_masked_2d\"] = \"\"\n",
    "\n",
    "# # take cum and non zero or nan indices to reconstruct pixels we want to use in ICA\n",
    "# # then clip these reconstructed pixels using the shp geodataframe\n",
    "# for index, row in frames_gdf.iterrows():\n",
    "#     frame = row['frame']\n",
    "#     cum_masked = row['cum_masked']\n",
    "#     lon_plot = row['lon']\n",
    "#     lat_plot = row['lat']\n",
    "#     cum = row['cum']\n",
    "\n",
    "#     # Create meshgrid of lon and lat\n",
    "#     lon, lat = np.meshgrid(lon_plot, lat_plot)\n",
    "\n",
    "#     # Flatten lon and lat\n",
    "#     lon_1d = lon.flatten()\n",
    "#     lat_1d = lat.flatten()\n",
    "\n",
    "#     # Create a GeoDataFrame for the flattened lon and lat\n",
    "#     geometry = [Point(lon, lat) for lon, lat in zip(lon_1d, lat_1d)]\n",
    "#     gdf_points = gpd.GeoDataFrame(geometry=geometry, columns=['geometry'])\n",
    "#     gdf_points.crs = \"EPSG:4326\"\n",
    "#     gdf_points['Latitude'] = lat_1d\n",
    "#     gdf_points['Longitude'] = lon_1d\n",
    "\n",
    "#     # Perform Spatial Join- i.e. clip points to save those within the polygons\n",
    "#     joined = gpd.sjoin(gdf_points, gdf_polygons, predicate='within')\n",
    "\n",
    "#     extracted_coordinates = joined[['Latitude', 'Longitude']]\n",
    "\n",
    "#     extracted_indices = joined.index\n",
    "\n",
    "#     # Create a boolean mask for lon and lat arrays\n",
    "#     mask = np.isin(np.arange(lon.size), extracted_indices)\n",
    "\n",
    "#     #mask_reshape=mask.reshape(cum_masked.shape[1:])\n",
    "#     #print(mask_reshape.shape)\n",
    "    \n",
    "#     # mask cum with the polygon mask\n",
    "#     masked_cum = cum_masked * np.where(mask, 1, np.nan)\n",
    "            \n",
    "#     frames_gdf.at[index, 'cum_polygon_masked_3d'] = masked_cum\n",
    "#     frames_gdf.at[index, 'cum_polygon_masked_2d'] = masked_cum.reshape(cum.shape[0], cum.shape[1] * cum.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e2295f-7d95-4961-9bbf-97a5fbafcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_pixels (1084164,)\n",
      "cum_no_zeros (261, 1084163)\n",
      "nan_indices (2931093, 2)\n",
      "nan_pixels (1084163,)\n",
      "cum_no_zeros_no_nans (261, 1069848)\n",
      "zero_pixels (1086168,)\n",
      "cum_no_zeros (183, 1086167)\n",
      "nan_indices (1732, 2)\n",
      "nan_pixels (1086167,)\n",
      "cum_no_zeros_no_nans (183, 1084806)\n"
     ]
    }
   ],
   "source": [
    "# Find columns (pixels) containing only zeros in the t-s and mask them\n",
    "\n",
    "frames_gdf[\"cum_no_nans_zeros\"] = \"\"\n",
    "frames_gdf[\"non_zero_ind\"] = \"\"\n",
    "frames_gdf[\"non_nan_ind\"] = \"\"\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_masked = row['cum_masked']\n",
    "\n",
    "    zero_pixels = np.all(cum_masked == 0, axis=0)\n",
    "    print('zero_pixels', zero_pixels.shape)\n",
    "    \n",
    "    # Create a new data array without columns (pixels) containing all zeros\n",
    "    cum_no_zeros = cum_masked[:, ~zero_pixels]\n",
    "    print('cum_no_zeros', cum_no_zeros.shape)\n",
    "    \n",
    "    # Print how many NaNs there are\n",
    "    nan_indices = np.argwhere(np.isnan(cum_no_zeros))\n",
    "    print('nan_indices', nan_indices.shape)\n",
    "\n",
    "    # find columns containing NaNs and zeroes\n",
    "    nan_pixels = np.any(np.isnan(cum_no_zeros), axis=0)\n",
    "    print('nan_pixels', nan_pixels.shape)\n",
    "\n",
    "    # create a new data array without nan columns (pixels)\n",
    "    cum_no_zeros_no_nans = cum_no_zeros[:, ~nan_pixels]\n",
    "    print('cum_no_zeros_no_nans', cum_no_zeros_no_nans.shape)\n",
    "\n",
    "    zero_ind = np.argwhere(zero_pixels).flatten()\n",
    "    non_zero_ind = np.argwhere(~zero_pixels).flatten()\n",
    "\n",
    "    nans = np.any(np.isnan(cum_masked), axis=0)\n",
    "    nan_ind = np.argwhere(nans).flatten()\n",
    "    non_nan_ind = np.argwhere(~nans).flatten()\n",
    "\n",
    "    frames_gdf.at[index, 'cum_no_nans_zeros'] = cum_no_zeros_no_nans\n",
    "    frames_gdf.at[index, 'non_zero_ind'] = non_zero_ind\n",
    "    frames_gdf.at[index, 'non_nan_ind'] = non_nan_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e20e2-b29e-408c-9acf-da1a474da90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 1069848)\n"
     ]
    }
   ],
   "source": [
    "frames_gdf[\"S_ft\"] = \"\"\n",
    "frames_gdf[\"restored_signals\"] = \"\"\n",
    "frames_gdf[\"restored_signals_3d\"] = \"\"\n",
    "\n",
    "# attempt ICA\n",
    "ncomponents=4\n",
    "\n",
    "for index, row in tqdm(frames_gdf.iterrows()):\n",
    "    frame = row['frame']\n",
    "    data = row['cum_no_nans_zeros']\n",
    "    non_nan_ind = row['non_nan_ind']\n",
    "    non_zero_ind = row['non_zero_ind']\n",
    "    cum = row['cum']\n",
    "\n",
    "    # Check the number of features\n",
    "    if data.shape[1] > ncomponents and data.shape[0] > ncomponents:\n",
    "        print(data.shape)\n",
    "    # Perform ICA\n",
    "        # set up the transformer (ica). In MATLAB you do the whitening first then the transforming, here you do it in one.\n",
    "        ica = FastICA(n_components=ncomponents, whiten=\"unit-variance\")\n",
    "    \n",
    "        # fit the transformer to the data array\n",
    "        S_ft = ica.fit_transform(data) # fit model and recover signals\n",
    "        #S_t = ica.transform(data) # recover sources from x using unmixing matrix\n",
    "        ## S_ft and S_t results should be identical as ica.transform uses mixing matrix calculated by ica.fit_trcd ansform\n",
    "    \n",
    "        frames_gdf.at[index, 'S_ft'] = S_ft\n",
    "    \n",
    "        # Take each signal and restore with outer product\n",
    "        restored_signals_outer = []\n",
    "        three_d_list = []\n",
    "        for j in range(ncomponents):\n",
    "            S_j = np.copy(S_ft)\n",
    "            signal = S_j[:,j]\n",
    "            mixing = ica.mixing_[:,j]\n",
    "            restored_signal_j = np.outer(signal, mixing)\n",
    "            \n",
    "            # Append the restored signal to the list\n",
    "            restored_signals_outer.append(restored_signal_j)\n",
    "    \n",
    "            # reshape restored signal into 3d for later use\n",
    "            # Convert common_indices to 3D indices i.e. flat indices into a tuple for 3d array\n",
    "            # first find indices common to non nan and non zero for data population\n",
    "            common_indices = np.intersect1d(non_nan_ind, non_zero_ind)\n",
    "    \n",
    "            # create a shape the same size as original data with nans\n",
    "            cum_with_nans = np.full((cum.shape[0], cum.shape[1], cum.shape[2]), np.nan)\n",
    "    \n",
    "            # Convert common_indices to 3D indices i.e. flat indices into a tuple for 3d array\n",
    "            indices_3d = np.unravel_index(common_indices, cum.shape[1:])\n",
    "    \n",
    "            # Create a copy of cum_with_nans to work with for each restored_signal\n",
    "            cum_with_nans_copy = cum_with_nans.copy()\n",
    "            \n",
    "            # need to make sure a different time series assigned to timestep\n",
    "            for m in range(restored_signal_j.shape[0]):\n",
    "                # Assign values from the restored signal to non-NaN positions\n",
    "                cum_with_nans_copy[m, indices_3d[0], indices_3d[1]] = restored_signal_j[m,:]\n",
    "    \n",
    "            # reshape cum_with_nans into time x pixels\n",
    "            cum_with_nans_pix = cum_with_nans_copy.reshape(cum.shape[0], cum.shape[1] * cum.shape[2])\n",
    "    \n",
    "            #reshape cum_with_nans_pix into 3d to save signals\n",
    "            restored_signal_3d = cum_with_nans_pix.reshape(cum.shape[0], cum.shape[1], cum.shape[2])\n",
    "            three_d_list.append(restored_signal_3d)\n",
    "            \n",
    "        frames_gdf.at[index, 'restored_signals'] = restored_signals_outer   \n",
    "        frames_gdf.at[index, 'restored_signals_3d'] = three_d_list\n",
    "    else:\n",
    "        print(\"Skipping ICA for frame {} because the number of features is not greater than {}.\".format(frame, ncomponents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18424dd0-13bd-4487-95d2-aac0db3e245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common indices\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    non_nan_ind = row['non_nan_ind']\n",
    "    non_zero_ind = row['non_zero_ind']\n",
    "    restored_signals_outer = row['restored_signals']\n",
    "    cum = row['cum']\n",
    "    lon_plot = row['lon']\n",
    "    lat_plot = row['lat']\n",
    "    common_indices = np.intersect1d(non_nan_ind, non_zero_ind)\n",
    "    \n",
    "    # # Create subplots\n",
    "    fig, axes = plt.subplots(ncomponents, 1, figsize=(12, 20))\n",
    "    # # Plot each restored signal in lat, lon on a separate subplot\n",
    "    for i, (restored_signal, ax) in enumerate(zip(restored_signals_outer, axes), start=1):\n",
    "        # Create a new matrix with NaNs\n",
    "        cum_with_nans = np.full((cum.shape[2] * cum.shape[1],), np.nan)\n",
    "\n",
    "         # Assign values from the restored signal to non-NaN positions\n",
    "        cum_with_nans[common_indices] = restored_signal[-1]\n",
    "        cum_with_nans_reshaped = cum_with_nans.reshape((cum.shape[1], cum.shape[2]))\n",
    "        lon, lat = np.meshgrid(lon_plot, lat_plot)\n",
    "\n",
    "        # Plot on the subplot\n",
    "        im = axes[i - 1].imshow(cum_with_nans_reshaped, extent=(lon.min(), lon.max(), lat.min(), lat.max()), cmap='viridis', interpolation ='none')\n",
    "        axes[i - 1].set_title(f'Restored Signal {i}')\n",
    "       # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[i - 1], label='mm/yr')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b7ad0-70e5-4f32-a0d3-46d6ee94d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(ncomponents, 1, figsize=(8, 2 * ncomponents), sharex=True)\n",
    "\n",
    "row_n = frames_gdf.iloc[1]\n",
    "\n",
    "frame = row_n['frame']\n",
    "restored_signals_outer = row_n['restored_signals']\n",
    "dates = row_n['dates']\n",
    "for i, restored_signal in enumerate(restored_signals_outer, start=1):\n",
    "    axes[i - 1].grid()\n",
    "    axes[i - 1].plot(dates, restored_signal)\n",
    "    axes[i - 1].set_title(f\"Restored Signal {i}\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a97f22-ab21-458f-b1c7-085b166d0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to solve for frequency and thus period of signals\n",
    "def sine_function(x, f):\n",
    "    return np.sin(2 * np.pi * f * x)\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    dates = row['dates']\n",
    "    restored_signals_3d = row['restored_signals_3d']\n",
    "    restored_signals = row['restored_signals']\n",
    "    corner_lat = row['corner_lat']\n",
    "    corner_lon = row['corner_lon']\n",
    "    post_lon = row['post_lon']\n",
    "    post_lat = row['post_lat']\n",
    "    width = row['width']\n",
    "    height = row['length']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    nc_data = row['imdates']\n",
    "\n",
    "    # Only proceed if there are enough coherent pixels\n",
    "    if restored_signals[0].shape[1] >= 20:\n",
    "        # List to store R-squared values for each trend\n",
    "        mean_gradient = []\n",
    "        median_r_squared = []\n",
    "        median_period = []\n",
    "        mean_second_derivative = []\n",
    "\n",
    "        # Convert dates to numerical values\n",
    "        num_dates = date2num(dates)\n",
    "\n",
    "        for m, signal in enumerate(restored_signals):\n",
    "            r_squared_values = []\n",
    "            gradient = []\n",
    "            second_deriv = []\n",
    "            periods = []\n",
    "\n",
    "            # Loop through each trend at each pixel\n",
    "            for i in range(signal.shape[1]):\n",
    "                # Perform linear regression and calculate R-squared\n",
    "                slope, _, r_value, _, _ = linregress(num_dates, signal[:, i])\n",
    "\n",
    "                # Store the gradient at each pixel\n",
    "                gradient.append(slope)\n",
    "\n",
    "                # Append R-squared value to the list\n",
    "                r_squared_values.append(r_value ** 2)\n",
    "\n",
    "                # Calculate the second derivative using numpy.gradient at each pixel\n",
    "                first_derivative = np.gradient(signal[:, i], num_dates)\n",
    "                second_derivative = np.gradient(first_derivative, num_dates)\n",
    "                second_derivative_mean = np.mean(second_derivative)\n",
    "                second_deriv.append(second_derivative_mean)\n",
    "\n",
    "                try:\n",
    "                    # fit signal with sine function\n",
    "                    popt, pcov = curve_fit(sine_function, num_dates, signal[:, i], p0=((1/365.25)), maxfev=1000)\n",
    "                    optimal_frequency = popt[0]\n",
    "                    period = 1 / optimal_frequency\n",
    "                    periods.append(period)\n",
    "                except (RuntimeError, OptimizeWarning) as e:\n",
    "                    print(f\"Curve fitting did not converge for signal {i}. Skipping...\")\n",
    "                    periods.append(None)\n",
    "                    \n",
    "            # take mean of gradients\n",
    "            mean_gradient.append(np.mean(gradient))\n",
    "            # take median of r_squared per IC\n",
    "            median_r_squared.append(np.median(r_squared_values))\n",
    "            # find median period of signal\n",
    "            median_period.append(np.median(period))\n",
    "            # take mean of second derivative per IC\n",
    "            mean_second_derivative.append(np.mean(second_deriv))\n",
    "\n",
    "            # plot components in space and time with parameters\n",
    "            plt.figure(figsize=(16,5))\n",
    "            plt.subplot2grid((1, 3), (0, 0))\n",
    "            plt.plot(dates,signal)\n",
    "            # Adding text to the top right of the plot\n",
    "            plt.text(0.95, 0.95, f\"Gradient: {np.mean(gradient):.4f}\\nR Squared: {np.median(r_squared_values):.4f}\\nPeriod: {np.median(period):.4f}\\nSecond Derivative: {np.mean(second_deriv):.4f}\",\n",
    "                 horizontalalignment='right',\n",
    "                 verticalalignment='top',\n",
    "                 transform=plt.gca().transAxes,\n",
    "                 bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "            plt.subplot2grid((1, 3), (0, 1))\n",
    "            plt.imshow(restored_signals_3d[m][-1,:,:], cmap='viridis', interpolation='none', extent=[np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)])\n",
    "            cbar = plt.colorbar(label='mm/yr', shrink=0.6)\n",
    "            \n",
    "            plt.subplot2grid((1, 3), (0, 2))\n",
    "            plt.plot(dates,Sft[:,m], label = 'Component time series')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "        # If mean gradient is negative, find the index of the trend with the maximum R-squared value\n",
    "        negative_indices = [i for i, val in enumerate(mean_gradient) if val < 0]\n",
    "        if negative_indices:\n",
    "            median_r_squared_negative_gradients = [median_r_squared[i] for i in negative_indices]\n",
    "            max_r_squared_negative_grad = np.max(median_r_squared_negative_gradients)\n",
    "            max_index_in_median_r_squared = median_r_squared.index(max_r_squared_negative_grad)\n",
    "\n",
    "            # Choose the signal\n",
    "            inelastic_signal = restored_signals_3d[max_index_in_median_r_squared]\n",
    "            inelastic_signal_subsiding = restored_signals[max_index_in_median_r_squared]\n",
    "            print(mean_gradient)\n",
    "            print(median_r_squared)\n",
    "            print(median_period)\n",
    "            print(mean_second_derivative)\n",
    "\n",
    "            # Save as NetCDF\n",
    "            output_nc_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/inelastic_components/{ncomponents}_comp/\"\n",
    "            if not os.path.exists(output_nc_dir):\n",
    "                os.makedirs(output_nc_dir)\n",
    "\n",
    "            output_nc_path = os.path.join(output_nc_dir, f\"{frame}_inelastic_component_{ncomponents}.nc\")\n",
    "\n",
    "            with nc.Dataset(output_nc_path, 'w') as file:\n",
    "                # Create dimensions\n",
    "                file.createDimension('dates', inelastic_signal.shape[0])\n",
    "                file.createDimension('latitude', inelastic_signal.shape[1])\n",
    "                file.createDimension('longitude', inelastic_signal.shape[2])\n",
    "\n",
    "                # Create variables\n",
    "                time_var = file.createVariable('dates', 'f4', ('dates',))\n",
    "                lat_var = file.createVariable('latitude', 'f4', ('latitude',))\n",
    "                lon_var = file.createVariable('longitude', 'f4', ('longitude',))\n",
    "                data_var = file.createVariable('data', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                period_var = file.createVariable('period', 'f4')\n",
    "                second_derivative_var = file.createVariable('second_derivative', 'f4')\n",
    "\n",
    "                # Add data to variables\n",
    "                time_var[:] = nc_data\n",
    "                lat_var[:] = lat\n",
    "                lon_var[:] = lon\n",
    "                data_var[:] = inelastic_signal\n",
    "                period_var[:] = median_period[max_index_in_median_r_squared]\n",
    "                second_derivative_var[:] = mean_second_derivative[max_index_in_median_r_squared]\n",
    "\n",
    "            # Save as GeoTIFF\n",
    "            output_tif_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/inelastic_components/{ncomponents}_comp/\"\n",
    "            if not os.path.exists(output_tif_dir):\n",
    "                os.makedirs(output_tif_dir)\n",
    " \n",
    "            output_tif_path = os.path.join(output_tif_dir, f\"{frame}_inelastic_component_{ncomponents}.tif\")\n",
    "            # Create a transformation for the GeoTIFF\n",
    "            post_lat_pos = post_lat*(-1)\n",
    "            transform = from_origin(corner_lon, corner_lat, post_lon, post_lat_pos)\n",
    "\n",
    "            with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                               dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                # Write the data to the GeoTIFF\n",
    "                dst.write(inelastic_signal[-1, :, :], 1)\n",
    "\n",
    "        # Save other signals as NetCDF\n",
    "        output_nc_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/other_components/{ncomponents}_comp/{frame}\"\n",
    "        if not os.path.exists(output_nc_dir):\n",
    "            os.makedirs(output_nc_dir)\n",
    "\n",
    "        output_nc_path = os.path.join(output_nc_dir, f\"other_component.nc\")\n",
    "\n",
    "        with nc.Dataset(output_nc_path, 'w') as file:\n",
    "            # Create dimensions\n",
    "            file.createDimension('dates', inelastic_signal.shape[0])\n",
    "            file.createDimension('latitude', inelastic_signal.shape[1])\n",
    "            file.createDimension('longitude', inelastic_signal.shape[2])\n",
    "\n",
    "            # Create variables\n",
    "            time_var = file.createVariable('dates', 'f4', ('dates',))\n",
    "            lat_var = file.createVariable('latitude', 'f4', ('latitude'))\n",
    "            lon_var = file.createVariable('longitude', 'f4', ('longitude'))\n",
    "            \n",
    "            # Create variables for each component\n",
    "            data_vars = []\n",
    "            period_vars = []\n",
    "            second_derivative_vars = []\n",
    "\n",
    "            for comp_index in range(ncomponents - 1):\n",
    "                data_var = file.createVariable(f'data_{comp_index}', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                period_var = file.createVariable(f'period_{comp_index}', 'f4')\n",
    "                second_derivative_var = file.createVariable(f'second_derivative_{comp_index}', 'f4')\n",
    "    \n",
    "                data_vars.append(data_var)\n",
    "                period_vars.append(period_var)\n",
    "                second_derivative_vars.append(second_derivative_var)\n",
    "\n",
    "            # Add data, period, and second derivative to variables\n",
    "            comp_index=0\n",
    "            for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                if i != max_index_in_median_r_squared:\n",
    "                    data_vars[comp_index][:] = signal_3d\n",
    "                    period_vars[comp_index][:] = median_period[i]\n",
    "                    second_derivative_vars[comp_index][:] = mean_second_derivative[i]\n",
    "                    comp_index += 1\n",
    "\n",
    "            time_var[:] = nc_data\n",
    "            lat_var[:] = lat\n",
    "            lon_var[:] = lon\n",
    "\n",
    "            # Save as GeoTIFF\n",
    "            output_tif_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/other_components/{ncomponents}_comp/{frame}\"\n",
    "\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(output_tif_dir):\n",
    "                # Iterate over the files in the directory\n",
    "                for filename in os.listdir(output_tif_dir):\n",
    "                # Check if the file is a GeoTIFF\n",
    "                    if filename.endswith(\".tif\"):\n",
    "                        # Construct the full path to the file\n",
    "                        file_path = os.path.join(output_tif_dir, filename)\n",
    "                        # Delete the file\n",
    "                        os.remove(file_path)\n",
    "            else:\n",
    "                print(\"Directory does not exist.\")\n",
    "                \n",
    "            for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                if i != max_index_in_median_r_squared:\n",
    "                    output_tif_path = os.path.join(output_tif_dir, f\"other_component_{i}.tif\")\n",
    "                    with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                                       dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                        # Write the data to the GeoTIFF\n",
    "                        dst.write(signal_3d[-1, :, :], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c863db-df1b-47ff-8b8a-1fb13dc785a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1a725-0682-4cae-98de-da0549f72a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambalics",
   "language": "python",
   "name": "mambalics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
