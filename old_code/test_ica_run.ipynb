{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56257907-39ad-4686-96dc-67cdd6393aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing modules\n",
      "loading files\n",
      "ica prep\n",
      "run ica\n",
      "checking for linearity\n",
      "median_r_squared: [0.002803804626299163, 0.02192258566842139, 0.009219935302496884, 0.9449565333517168]\n",
      "mean_gradient: [-0.0003597727675197191, -0.0009095698352198888, -0.0010323512108625509, -0.05469873305480624]\n",
      "median dominant period: [27.42857142857143, 14.76923076923077, 3.918367346938776, 192.0] timesteps - do not use this data yet\n",
      "mean_second_derivative: [0.00024230083253810983, 0.00026996367667101416, 0.0004930520363501414, -0.00030224777628160856] mm/yr^2\n",
      "Saving inelastic nc\n",
      "Saving inelastic geotiff\n",
      "Saving other component nc file\n",
      "Saving other component geotiffs\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "print(\"importing modules\") \n",
    "import os\n",
    "from sklearn.decomposition import FastICA, PCA \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from scipy.interpolate import interp2d\n",
    "from numpy.ma import masked_array\n",
    "import glob\n",
    "import h5py\n",
    "from datetime import datetime, timedelta \n",
    "import geopandas as gpd\n",
    "from scipy.stats import linregress\n",
    "from matplotlib.dates import date2num, num2date\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from rasterio.transform import from_origin\n",
    "import netCDF4 as nc\n",
    "import argparse\n",
    "\n",
    "# this is Andrew Watson's library of functions, see \n",
    "# https://github.com/Active-Tectonics-Leeds/interseismic_practical\n",
    "import sys\n",
    "import interseis_lib as lib\n",
    "\n",
    "# Parse command-line arguments\n",
    "# parser = argparse.ArgumentParser(description='Process a single frame.')\n",
    "# parser.add_argument('--frame', type=str, help='Frame name')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Use the provided frame name or default to 'frame1' (or any default frame name)\n",
    "#frame = args.frame\n",
    "frame=\"028A_05385_191813\"\n",
    "print(\"loading files\")\n",
    "\n",
    "# load files for frames\n",
    "cumh5_dir = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/cumh5\"\n",
    "mask_dir = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/mask\"\n",
    "EQA_dir = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/EQA.dem_par\"\n",
    "subs_poly_path = 'vU_merge_161023_noBabRas_WGS84_fillednosmooth_wmean_rad2_dist18_ltemin10_polygonised_dissolved.shp'\n",
    "\n",
    "ncomponents = 4\n",
    "\n",
    "output_nc_path = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/inelastic_components/{}_comp/{}_inelastic_component_{}.nc\".format(ncomponents, frame, ncomponents)\n",
    "output_tif_path = \"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/inelastic_components/{}_comp/{}_inelastic_component_{}.tif\".format(ncomponents, frame, ncomponents)\n",
    "\n",
    "frames_data = []\n",
    "EQA_par_pattern = os.path.join(EQA_dir,f\"{frame}_GEOCml*GACOSmask_EQA.dem_par\")\n",
    "EQA_par_file = glob.glob(EQA_par_pattern)\n",
    "cumh5_pattern = os.path.join(cumh5_dir,f\"{frame}_GEOCml*GACOSmask_cum.h5\")\n",
    "cumh5_file = glob.glob(cumh5_pattern)\n",
    "mask_pattern = os.path.join(mask_dir,f\"{frame}_GEOCml*GACOSmask_coh_03_mask.geo.tif\") \n",
    "mask_file = glob.glob(mask_pattern)\n",
    "\n",
    "with h5py.File(cumh5_file[0], 'r') as file:\n",
    "    imdates = file['imdates']\n",
    "    imdates = imdates[:]\n",
    "    vel = file['vel']\n",
    "    vel = vel[:]\n",
    "    cum = file['cum']\n",
    "    cum = cum[:]\n",
    "    \n",
    "dates=[]\n",
    "for date_value in imdates:\n",
    "    date_string = str(date_value) # Convert int32 to string\n",
    "    year = int(date_string[:4])\n",
    "    month = int(date_string[4:6])\n",
    "    day = int(date_string[6:])\n",
    "    real_date = datetime(year, month, day)\n",
    "    dates.append(real_date)\n",
    "    width = int(lib.get_par(EQA_par_file[0],'width'))\n",
    "    length = int(lib.get_par(EQA_par_file[0],'nlines'))\n",
    "    \n",
    "# get corner positions\n",
    "corner_lat = float(lib.get_par(EQA_par_file[0], 'corner_lat'))\n",
    "corner_lon = float(lib.get_par(EQA_par_file[0], 'corner_lon'))\n",
    "\n",
    "# get post spacing (distance between velocity measurements)\n",
    "post_lat = float(lib.get_par(EQA_par_file[0],'post_lat'))\n",
    "post_lon = float(lib.get_par(EQA_par_file[0],'post_lon'))\n",
    "\n",
    "# calculate grid spacings\n",
    "lat = corner_lat + post_lat*np.arange(1,length+1) - post_lat/2\n",
    "lon = corner_lon + post_lon*np.arange(1,width+1) - post_lon/2\n",
    "frames_data.append({\n",
    "    'frame': frame,\n",
    "    'EQA_file': EQA_par_file,\n",
    "    'cumh5_file': cumh5_file,\n",
    "    'mask_file': mask_file,\n",
    "    'imdates': imdates,\n",
    "    'vel': vel,\n",
    "    'cum': cum,\n",
    "    'dates': dates,\n",
    "    'width': width,\n",
    "    'length': length,\n",
    "    'corner_lat': corner_lat,\n",
    "    'corner_lon': corner_lon,\n",
    "    'post_lat': post_lat,\n",
    "    'post_lon': post_lon,\n",
    "    'lat': lat,\n",
    "    'lon': lon\n",
    "        })\n",
    "\n",
    "# Create GeoDataFrame\n",
    "frames_gdf = gpd.GeoDataFrame(frames_data, columns=['frame', 'EQA_file', 'cumh5_file', 'mask_file', 'imdates', 'vel', 'cum', 'dates', 'width', 'length', 'corner_lat', 'corner_lon', 'post_lat', 'post_lon','lat', 'lon']) \n",
    "\n",
    "print(\"ica prep\")\n",
    "# cum shape is (t, lat, lon) we want to make an array of (pixels, time) we want to reshape cum (202, 268, 327) \n",
    "# into (202,(268*327))\n",
    "frames_gdf[\"cum_\"] = \"\"\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_data = row['cum']\n",
    "    # Check if 'cum' data is not empty\n",
    "    if not np.isnan(cum_data).all():\n",
    "        cum_shape = cum_data.shape\n",
    "        n_pix = cum_shape[1] * cum_shape[2]\n",
    "        cum_ = np.reshape(cum_data, (cum_shape[0], n_pix))\n",
    "        frames_gdf.at[index, 'cum_'] = cum_\n",
    "        \n",
    "# we have cum of shape 202, 268, 327 and mask tif. We want to mask cum with mask tif i.e. make pixels in cum nan \n",
    "# where mask_tif = 0. NaNs are where no data e.g. outside of frame, low coh...\n",
    "frames_gdf[\"cum_masked\"] = \"\"\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_data = row['cum_']\n",
    "    mask = row['mask_file']\n",
    "    with rasterio.open(mask[0]) as tif:\n",
    "        # Read the raster data\n",
    "        mask_tif = tif.read(1)\n",
    "        # Reshape the mask to 1D\n",
    "        mask_1d = mask_tif.flatten()\n",
    "        # Tile the first row along the rows to match the shape of asc_cum_\n",
    "        mask_2d = np.tile(mask_1d, (cum_data.shape[0],1))\n",
    "        \n",
    "        # Apply the mask to every element in the 3D array\n",
    "        cum_masked = cum_data * mask_2d\n",
    "        frames_gdf.at[index, 'cum_masked'] = cum_masked\n",
    "        \n",
    "# Find columns (pixels) containing zeros\n",
    "frames_gdf[\"cum_no_nans_zeros\"] = \"\"\n",
    "frames_gdf[\"non_zero_ind\"] = \"\"\n",
    "frames_gdf[\"non_nan_ind\"] = \"\"\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_masked = row['cum_masked']\n",
    "    zero_pixels = np.all(cum_masked == 0, axis=0)\n",
    "    # Create a new data array without columns (pixels) containing all zeros\n",
    "    cum_no_zeros = cum_masked[:, ~zero_pixels]\n",
    "    # Print how many NaNs there are\n",
    "    nan_indices = np.argwhere(np.isnan(cum_no_zeros))\n",
    "    # find columns containing NaNs and zeroes\n",
    "    nan_pixels = np.any(np.isnan(cum_no_zeros), axis=0)\n",
    "    # create a new data array without nan columns (pixels)\n",
    "    cum_no_zeros_no_nans = cum_no_zeros[:, ~nan_pixels]\n",
    "    zero_ind = np.argwhere(zero_pixels).flatten()\n",
    "    non_zero_ind = np.argwhere(~zero_pixels).flatten()\n",
    "    nans = np.any(np.isnan(cum_masked), axis=0)\n",
    "    nan_ind = np.argwhere(nans).flatten()\n",
    "    non_nan_ind = np.argwhere(~nans).flatten()\n",
    "    frames_gdf.at[index, 'cum_no_nans_zeros'] = cum_no_zeros_no_nans\n",
    "    frames_gdf.at[index, 'non_zero_ind'] = non_zero_ind\n",
    "    frames_gdf.at[index, 'non_nan_ind'] = non_nan_ind\n",
    "\n",
    "print(\"run ica\")\n",
    "frames_gdf[\"S_ft\"] = \"\" \n",
    "frames_gdf[\"restored_signals\"] = \"\"\n",
    "\n",
    "# attempt ICA\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    data = row['cum_no_nans_zeros']\n",
    "    # set up the transformer (ica). In MATLAB you do the whitening first then the transforming, here you do it in \n",
    "    # one.\n",
    "    ica = FastICA(n_components=ncomponents, whiten=\"unit-variance\")\n",
    "    \n",
    "    # fit the transformeter to the data array\n",
    "    try:\n",
    "        S_ft = ica.fit_transform(data) # fit model and recover signals\n",
    "        S_t = ica.transform(data) # recover sources from x using unmixing matrix\n",
    "    except ConvergenceWarning as e:\n",
    "        print(\"ICA did not converge for frame:\", frame)\n",
    "        raise e\n",
    "    \n",
    "    ## S_ft and S_t results should be identical as ica.transform uses mixing matrix calculated by \n",
    "    ## ica.fit_transform\n",
    "    frames_gdf.at[index, 'S_ft'] = S_ft\n",
    "    # Take each signal and restore with outer product\n",
    "    restored_signals_outer = []\n",
    "    for j in range(ncomponents):\n",
    "        S_j = np.copy(S_ft)\n",
    "        signal = S_j[:,j]\n",
    "        mixing = ica.mixing_[:,j]\n",
    "        restored_signal_j = np.outer(signal, mixing)\n",
    "        \n",
    "        # Append the restored signal to the list\n",
    "        restored_signals_outer.append(restored_signal_j)\n",
    "        \n",
    "    frames_gdf.at[index, 'restored_signals'] = restored_signals_outer\n",
    "    \n",
    "# Find the common indices\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    non_nan_ind = row['non_nan_ind']\n",
    "    non_zero_ind = row['non_zero_ind']\n",
    "    restored_signals_outer = row['restored_signals']\n",
    "    cum = row['cum']\n",
    "    lon_plot = row['lon']\n",
    "    lat_plot = row['lat']\n",
    "    common_indices = np.intersect1d(non_nan_ind, non_zero_ind)\n",
    "    \n",
    "    # do some reshaping\n",
    "    for restored_signal in restored_signals_outer:\n",
    "        # Create a new matrix with NaNs\n",
    "        cum_with_nans = np.full((cum.shape[2] * cum.shape[1],), np.nan)\n",
    "         # Assign values from the restored signal to non-NaN positions\n",
    "        cum_with_nans[common_indices] = restored_signal[-1]\n",
    "        cum_with_nans_reshaped = cum_with_nans.reshape((cum.shape[1], cum.shape[2]))\n",
    "print(\"checking for linearity\")\n",
    "\n",
    "# keep only subsiding polygon pixels from cum_nozeros_no_nans convert polygons shape into a geopandas dataframe\n",
    "frames_gdf[\"subsiding_restored_signals\"] = \"\"\n",
    "frames_gdf[\"restored_signals_3d\"] = \"\"\n",
    "gdf_polygons = gpd.read_file(subs_poly_path)\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    non_nan_ind = row['non_nan_ind']\n",
    "    non_zero_ind = row['non_zero_ind']\n",
    "    restored_signals_outer = row['restored_signals']\n",
    "    cum = row['cum']\n",
    "    lon_plot = row['lon']\n",
    "    lat_plot = row['lat']\n",
    "    common_indices = np.intersect1d(non_nan_ind, non_zero_ind)\n",
    "    cum_with_nans = np.full((cum.shape[0], cum.shape[1], cum.shape[2]), np.nan)\n",
    "    \n",
    "    # Create meshgrid of lon and lat\n",
    "    lon, lat = np.meshgrid(lon_plot, lat_plot)\n",
    "    \n",
    "    # Flatten lon and lat\n",
    "    lon_1d = lon.flatten()\n",
    "    lat_1d = lat.flatten()\n",
    "    \n",
    "    # Create a GeoDataFrame for the flattened lon and lat\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(lon_1d, lat_1d)]\n",
    "    gdf_points = gpd.GeoDataFrame(geometry=geometry, columns=['geometry'])\n",
    "    gdf_points.crs = \"EPSG:4326\"\n",
    "    gdf_points['Latitude'] = lat_1d\n",
    "    gdf_points['Longitude'] = lon_1d\n",
    "    \n",
    "    # Perform Spatial Join\n",
    "    joined = gpd.sjoin(gdf_points, gdf_polygons, predicate='within')\n",
    "    extracted_coordinates = joined[['Latitude', 'Longitude']]\n",
    "    extracted_indices = joined.index\n",
    "    \n",
    "    # Create a boolean mask for lon and lat arrays\n",
    "    mask = np.isin(np.arange(lon.size), extracted_indices)\n",
    "    subsiding_restored = []\n",
    "    three_d_list = []\n",
    "    \n",
    "    for restored_signal in restored_signals_outer:\n",
    "        # Convert common_indices to 3D indices i.e. flat indices into a tuple for 3d array\n",
    "        indices_3d = np.unravel_index(common_indices, cum.shape[1:])\n",
    "        # Create a copy of cum_with_nans to work with for each restored_signal\n",
    "        cum_with_nans_copy = cum_with_nans.copy()\n",
    "        \n",
    "        # need to make sure a different time series assigned to timestep\n",
    "        for j in range(restored_signal.shape[0]):\n",
    "            \n",
    "            # Assign values from the restored signal to non-NaN positions\n",
    "            cum_with_nans_copy[j, indices_3d[0], indices_3d[1]] = restored_signal[j,:]\n",
    "        \n",
    "        # reshape cum_with_nans into time x pixels\n",
    "        cum_with_nans_pix = cum_with_nans_copy.reshape(cum.shape[0], cum.shape[1] * cum.shape[2])\n",
    "        \n",
    "        #reshape cum_with_nans_pix into 3d to save signals\n",
    "        restored_signal_3d = cum_with_nans_pix.reshape(cum.shape[0], cum.shape[1], cum.shape[2])\n",
    "        three_d_list.append(restored_signal_3d)\n",
    "        \n",
    "        # mask cum_with_nans_pix with extracted indices\n",
    "        masked_cum_with_nans_pix = cum_with_nans_pix * np.where(mask == 0, np.nan, 1)\n",
    "        subsiding_restored.append(masked_cum_with_nans_pix)\n",
    "        \n",
    "    frames_gdf.at[index, 'subsiding_restored_signals'] = subsiding_restored\n",
    "    frames_gdf.at[index, \"restored_signals_3d\"] = three_d_list\n",
    "\n",
    "# remove nans and zeros from each subsiding restored signal\n",
    "frames_gdf[\"subsiding_no_nans\"] = \"\"\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    subsiding_restored = row['subsiding_restored_signals']\n",
    "    no_nans_store = []\n",
    "    for restored_signal in subsiding_restored:\n",
    "        # Print how many NaNs there are\n",
    "        nan_indices = np.argwhere(np.isnan(restored_signal))\n",
    "        # find columns containing NaNs and zeroes\n",
    "        nan_pixels = np.any(np.isnan(restored_signal), axis=0)\n",
    "        # create a new data array without nan columns (pixels)\n",
    "        subsiding_no_nans = restored_signal[:, ~nan_pixels]\n",
    "        no_nans_store.append(subsiding_no_nans)\n",
    "    frames_gdf.at[index, 'subsiding_no_nans'] = no_nans_store\n",
    "\n",
    "# find most linear!!\n",
    "frames_gdf[\"inelastic_restored_signal_3d\"] = \"\"\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    restored_signals = row['subsiding_no_nans']\n",
    "    dates = row['dates']\n",
    "    restored_signals_3d = row['restored_signals_3d']\n",
    "    restored_signals_full = row['restored_signals']\n",
    "    corner_lat = row['corner_lat']\n",
    "    corner_lon = row['corner_lon']\n",
    "    post_lon = row['post_lon']\n",
    "    post_lat = row['post_lat']\n",
    "    width = row['width']\n",
    "    height = row['length']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    nc_data = row['imdates']\n",
    "    \n",
    " # Only proceed if there are enough coherent pixels\n",
    "    if restored_signals[0].shape[1] >= 20:\n",
    "        # List to store R-squared values for each trend\n",
    "        mean_gradient = []\n",
    "        median_r_squared = []\n",
    "        median_dominant_period = []\n",
    "        mean_second_derivative = []\n",
    "\n",
    "        # Convert dates to numerical values\n",
    "        num_dates = date2num(dates)\n",
    "\n",
    "        for signal in restored_signals:\n",
    "            r_squared_values = []\n",
    "            gradient = []\n",
    "            second_deriv = []\n",
    "            dominant_periods = []\n",
    "\n",
    "            # Loop through each trend at each pixel\n",
    "            for i in range(signal.shape[1]):\n",
    "                # Perform linear regression and calculate R-squared\n",
    "                slope, _, r_value, _, _ = linregress(num_dates, signal[:, i])\n",
    "\n",
    "                # Store the gradient at each pixel\n",
    "                gradient.append(slope)\n",
    "\n",
    "                # Append R-squared value to the list\n",
    "                r_squared_values.append(r_value ** 2)\n",
    "\n",
    "                # Calculate the second derivative using numpy.gradient at each pixel\n",
    "                first_derivative = np.gradient(signal[:, i], num_dates)\n",
    "                second_deriv.append(np.gradient(first_derivative, num_dates))\n",
    "\n",
    "                # Perform FFT and find frequencies\n",
    "                fft_result = np.fft.fft(signal[:, i])\n",
    "\n",
    "                # Calculate the corresponding frequency using FFT frequencies\n",
    "                freqs = np.fft.fftfreq(len(signal[:, i]))\n",
    "                dominant_frequency = np.abs(freqs[np.argmax(np.abs(fft_result))])\n",
    "                dominant_period = 1 / dominant_frequency\n",
    "                dominant_periods.append(dominant_period)\n",
    "\n",
    "            # take mean of gradients\n",
    "            mean_gradient.append(np.mean(gradient))\n",
    "            # take median of r_squared per IC\n",
    "            median_r_squared.append(np.median(r_squared_values))\n",
    "            # find median period of signal\n",
    "            median_dominant_period.append(np.median(dominant_periods))\n",
    "            # take mean of second derivative per IC\n",
    "            mean_second_derivative.append(np.mean(second_deriv))\n",
    "    \n",
    "        print('median_r_squared:', median_r_squared)\n",
    "        print('mean_gradient:', mean_gradient)\n",
    "        print('median dominant period:', median_dominant_period, 'timesteps - do not use this data yet')\n",
    "        print('mean_second_derivative:', mean_second_derivative, 'mm/yr^2')\n",
    "    \n",
    "        # If mean gradient is negative, find the index of the trend with the maximum R-squared value\n",
    "        negative_indices = [i for i, val in enumerate(mean_gradient) if val < 0]\n",
    "        if negative_indices:\n",
    "            median_r_squared_negative_gradients = [median_r_squared[i] for i in negative_indices]\n",
    "            max_r_squared_negative_grad = np.max(median_r_squared_negative_gradients)\n",
    "            max_index_in_median_r_squared = median_r_squared.index(max_r_squared_negative_grad)\n",
    "\n",
    "            # Choose the signal\n",
    "            inelastic_signal = restored_signals_3d[max_index_in_median_r_squared]\n",
    "            inelastic_signal_subsiding = restored_signals[max_index_in_median_r_squared]\n",
    "\n",
    "            # Save as NetCDF\n",
    "            output_nc_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/inelastic_components/{ncomponents}_comp/\"\n",
    "            if not os.path.exists(output_nc_dir):\n",
    "                os.makedirs(output_nc_dir)\n",
    "\n",
    "            output_nc_path = os.path.join(output_nc_dir, f\"{frame}_inelastic_component_{ncomponents}.nc\")\n",
    "            \n",
    "            print('Saving inelastic nc')\n",
    "            with nc.Dataset(output_nc_path, 'w') as file:\n",
    "                # Create dimensions\n",
    "                file.createDimension('dates', inelastic_signal.shape[0])\n",
    "                file.createDimension('latitude', inelastic_signal.shape[1])\n",
    "                file.createDimension('longitude', inelastic_signal.shape[2])\n",
    "\n",
    "                # Create variables\n",
    "                time_var = file.createVariable('dates', 'f4', ('dates',))\n",
    "                lat_var = file.createVariable('latitude', 'f4', ('latitude',))\n",
    "                lon_var = file.createVariable('longitude', 'f4', ('longitude',))\n",
    "                data_var = file.createVariable('data', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                period_var = file.createVariable('period', 'f4')\n",
    "                second_derivative_var = file.createVariable('second_derivative', 'f4')\n",
    "\n",
    "                # Add data to variables\n",
    "                time_var[:] = nc_data\n",
    "                lat_var[:] = lat\n",
    "                lon_var[:] = lon\n",
    "                data_var[:] = inelastic_signal\n",
    "                period_var[:] = median_dominant_period[max_index_in_median_r_squared]\n",
    "                second_derivative_var[:] = mean_second_derivative[max_index_in_median_r_squared]\n",
    "            \n",
    "            print('Saving inelastic geotiff')\n",
    "            # Save as GeoTIFF\n",
    "            output_tif_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/inelastic_components/{ncomponents}_comp/\"\n",
    "            if not os.path.exists(output_tif_dir):\n",
    "                os.makedirs(output_tif_dir)\n",
    "\n",
    "            output_tif_path = os.path.join(output_tif_dir, f\"{frame}_inelastic_component_{ncomponents}.tif\")\n",
    "            flipped_data = np.flipud(inelastic_signal[-1, :, :])  # Assuming this is the correct data to flip\n",
    "            # Create a transformation for the GeoTIFF\n",
    "            transform = from_origin(corner_lon, corner_lat, post_lon, post_lat)\n",
    "\n",
    "            with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                               dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                # Write the data to the GeoTIFF\n",
    "                dst.write(flipped_data, 1)\n",
    "\n",
    "        # Save other signals as NetCDF\n",
    "        output_nc_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/other_components/{ncomponents}_comp/{frame}\"\n",
    "        if not os.path.exists(output_nc_dir):\n",
    "            os.makedirs(output_nc_dir)\n",
    "\n",
    "        output_nc_path = os.path.join(output_nc_dir, f\"other_component.nc\")\n",
    "\n",
    "        print('Saving other component nc file')\n",
    "        with nc.Dataset(output_nc_path, 'w') as file:\n",
    "            # Create dimensions\n",
    "            file.createDimension('dates', inelastic_signal.shape[0])\n",
    "            file.createDimension('latitude', inelastic_signal.shape[1])\n",
    "            file.createDimension('longitude', inelastic_signal.shape[2])\n",
    "\n",
    "            # Create variables\n",
    "            time_var = file.createVariable('dates', 'f4', ('dates',))\n",
    "            lat_var = file.createVariable('latitude', 'f4', ('latitude'))\n",
    "            lon_var = file.createVariable('longitude', 'f4', ('longitude'))\n",
    "            \n",
    "            # Create variables for each component\n",
    "            data_vars = []\n",
    "            period_vars = []\n",
    "            second_derivative_vars = []\n",
    "\n",
    "            for comp_index in range(ncomponents - 1):\n",
    "                data_var = file.createVariable(f'data_{comp_index}', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                period_var = file.createVariable(f'period_{comp_index}', 'f4')\n",
    "                second_derivative_var = file.createVariable(f'second_derivative_{comp_index}', 'f4')\n",
    "    \n",
    "                data_vars.append(data_var)\n",
    "                period_vars.append(period_var)\n",
    "                second_derivative_vars.append(second_derivative_var)\n",
    "\n",
    "            # Add data, period, and second derivative to variables\n",
    "            comp_index=0\n",
    "            for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                if i != max_index_in_median_r_squared:\n",
    "                    data_vars[comp_index][:] = signal_3d\n",
    "                    period_vars[comp_index][:] = median_dominant_period[i]\n",
    "                    second_derivative_vars[comp_index][:] = mean_second_derivative[i]\n",
    "                    comp_index += 1\n",
    "\n",
    "            time_var[:] = nc_data\n",
    "            lat_var[:] = lat\n",
    "            lon_var[:] = lon\n",
    "\n",
    "            print('Saving other component geotiffs')\n",
    "            # Save as GeoTIFF\n",
    "            output_tif_dir = f\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran/other_components/{ncomponents}_comp/{frame}\"\n",
    "\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(output_tif_dir):\n",
    "                # Iterate over the files in the directory\n",
    "                for filename in os.listdir(output_tif_dir):\n",
    "                # Check if the file is a GeoTIFF\n",
    "                    if filename.endswith(\".tif\"):\n",
    "                        # Construct the full path to the file\n",
    "                        file_path = os.path.join(output_tif_dir, filename)\n",
    "                        # Delete the file\n",
    "                        os.remove(file_path)\n",
    "            else:\n",
    "                print(\"Directory does not exist.\")\n",
    "                \n",
    "            for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                if i != max_index_in_median_r_squared:\n",
    "                    output_tif_path = os.path.join(output_tif_dir, f\"other_component_{i}.tif\")\n",
    "                    flipped_final_disp_other = np.flipud(signal_3d[-1, :, :])  # Assuming this is the correct data to flip\n",
    "\n",
    "                    with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                                       dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                        # Write the data to the GeoTIFF\n",
    "                        dst.write(flipped_final_disp_other, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5219f2-6c5c-4058-924b-5bb5bbee377e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambalics",
   "language": "python",
   "name": "mambalics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
