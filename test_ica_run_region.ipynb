{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183870c2-1a63-466f-a5b2-0a740cc00461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing modules\n",
      "loading files\n",
      "reshaping array\n",
      "masking array\n",
      "removing pixels with only zeros\n",
      "running ICA\n",
      "identifying elastic and inelastic components and save outputs\n",
      "Saving component figure at /home/users/eejap002/ica_scripts/component_plots/all_iran/spatial/3_components/028A_05385_191813/028A_05385_191813_component_0.png\n",
      "Saving component figure at /home/users/eejap002/ica_scripts/component_plots/all_iran/spatial/3_components/028A_05385_191813/028A_05385_191813_component_1.png\n",
      "Saving component figure at /home/users/eejap002/ica_scripts/component_plots/all_iran/spatial/3_components/028A_05385_191813/028A_05385_191813_component_2.png\n",
      "mean_gradient [0.00011792155014146773, -0.005247204258403366, -0.0008089333665980137]\n",
      "median_r_squared [0.0018432920102508209, 0.9894848248852783, 0.006162702522091443]\n",
      "median_mae [4.496591070101877, 0.2586462795850979, 5.791446927127793]\n",
      "mean_second_derivative [-0.00017760507079318677, 1.460757445323453e-05, -0.0004564527239532278]\n",
      "Saving inelastic component nc and tif at /gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran_data/outputs/archive/spatial_2016_2022/spatial/3_comp/028A_05385_191813\n",
      "Saving other component tifs at /gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran_data/outputs/archive/spatial_2016_2022/spatial/3_comp/028A_05385_191813\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "print(\"importing modules\")\n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from scipy.interpolate import interp2d\n",
    "from numpy.ma import masked_array\n",
    "import glob\n",
    "import h5py\n",
    "from datetime import datetime, timedelta\n",
    "import geopandas as gpd\n",
    "from scipy.stats import linregress\n",
    "from matplotlib.dates import date2num, num2date\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from rasterio.transform import from_origin\n",
    "import netCDF4 as nc\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import OptimizeWarning\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import shutil\n",
    "import argparse\n",
    "\n",
    "# this is Andrew Watson's library of functions, see https://github.com/Active-Tectonics-Leeds/interseismic_practical\n",
    "import sys\n",
    "import interseis_lib as lib\n",
    "\n",
    "#---------VARIABLES TO CHANGE--------#\n",
    "n_components=3\n",
    "out_dir=\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran_data/outputs/archive/spatial_2016_2022/\"\n",
    "data_dir=\"/gws/nopw/j04/nceo_geohazards_vol1/projects/COMET/eejap002/ica_data/all_iran_data/\"\n",
    "plot_dir=\"/home/users/eejap002/ica_scripts/component_plots/all_iran/\"\n",
    "polygon_dir=\"/home/users/eejap002/ica_scripts/polygons/\"\n",
    "approach=\"spatial\" # choose spatial or temporal\n",
    "\n",
    "print('loading files')\n",
    "cumh5_dir = os.path.join(data_dir,\"cumh5_2016_2022\")\n",
    "mask_dir = os.path.join(data_dir,\"mask_2016_2022\")\n",
    "EQA_dir = os.path.join(data_dir,\"EQA.dem_par_2016_2022\")\n",
    "subs_poly_path = os.path.join(data_dir,\"vU_merge_161023_noBabRas_WGS84_fillednosmooth_wmean_rad2_dist18_ltemin10_polygonised_dissolved.shp\")\n",
    "\n",
    "# Parse command-line arguments\n",
    "# parser = argparse.ArgumentParser(description='Process a Frame.')\n",
    "# parser.add_argument('--frame', type=str, help='Frame name')\n",
    "# args = parser.parse_args()\n",
    "# frame= args.frame\n",
    "\n",
    "#frame='Javin_159A_05389_131313'\n",
    "frame='028A_05385_191813'\n",
    "\n",
    "#-------------------------------------------#\n",
    "# Define the calc_model function\n",
    "def calc_model(dph, imdates_ordinal, model):\n",
    "    imdates_years = imdates_ordinal / 365.25  # Convert ordinal dates to years\n",
    "\n",
    "    # Construct design matrix A based on the selected model\n",
    "    A = sm.add_constant(imdates_years)  # Add constant term\n",
    "\n",
    "    if model == 1:  # Annual+L\n",
    "        sin = np.sin(2 * np.pi * imdates_years)\n",
    "        cos = np.cos(2 * np.pi * imdates_years)\n",
    "        A = np.concatenate((A, sin[:, np.newaxis], cos[:, np.newaxis]), axis=1)\n",
    "    elif model == 2:  # Quad\n",
    "        A = np.concatenate((A, (imdates_years ** 2)[:, np.newaxis]), axis=1)\n",
    "    elif model == 3:  # Annual+Q\n",
    "        sin = np.sin(2 * np.pi * imdates_years)\n",
    "        cos = np.cos(2 * np.pi * imdates_years)\n",
    "        A = np.concatenate((A, (imdates_years ** 2)[:, np.newaxis], sin[:, np.newaxis], cos[:, np.newaxis]),\n",
    "                           axis=1)\n",
    "\n",
    "    # Fit OLS model and predict values\n",
    "    result = sm.OLS(dph, A, missing='drop').fit()\n",
    "    yvalues = result.predict(A)\n",
    "\n",
    "    return yvalues\n",
    "\n",
    "#frame is region if running individual region\n",
    "frames_data = []\n",
    "\n",
    "EQA_par_pattern = os.path.join(EQA_dir,f\"*{frame}_GEOCml*GACOS*mask_EQA.dem_par\")\n",
    "EQA_par_file = glob.glob(EQA_par_pattern)\n",
    "cumh5_pattern = os.path.join(cumh5_dir,f\"*{frame}_GEOCml*GACOS*mask_cum.h5\")\n",
    "cumh5_file = glob.glob(cumh5_pattern)\n",
    "mask_pattern = os.path.join(mask_dir,f\"*{frame}_GEOCml*GACOS*mask_coh_03_mask.geo.tif\")\n",
    "mask_file = glob.glob(mask_pattern)\n",
    "\n",
    "with h5py.File(cumh5_file[0], 'r') as file:\n",
    "    imdates = file['imdates']\n",
    "    imdates = imdates[:] \n",
    "    vel = file['vel']\n",
    "    vel = vel[:]\n",
    "    cum = file['cum']\n",
    "    cum = cum[:]\n",
    "\n",
    "dates=[]\n",
    "for date_value in imdates:\n",
    "    date_string = str(date_value)  # Convert int32 to string\n",
    "    year = int(date_string[:4])\n",
    "    month = int(date_string[4:6])\n",
    "    day = int(date_string[6:])\n",
    "    real_date = datetime(year, month, day)\n",
    "    dates.append(real_date)    \n",
    "\n",
    "width = int(lib.get_par(EQA_par_file[0],'width'))\n",
    "length = int(lib.get_par(EQA_par_file[0],'nlines'))\n",
    "    \n",
    "# get corner positions\n",
    "corner_lat = float(lib.get_par(EQA_par_file[0], 'corner_lat'))\n",
    "corner_lon = float(lib.get_par(EQA_par_file[0], 'corner_lon'))\n",
    "\n",
    "# get post spacing (distance between velocity measurements)\n",
    "post_lat = float(lib.get_par(EQA_par_file[0],'post_lat'))\n",
    "post_lon = float(lib.get_par(EQA_par_file[0],'post_lon'))\n",
    "\n",
    "# calculate grid spacings\n",
    "lat = corner_lat + post_lat*np.arange(1,length+1) - post_lat/2\n",
    "lon = corner_lon + post_lon*np.arange(1,width+1) - post_lon/2\n",
    "\n",
    "frames_data.append({\n",
    "    'frame': frame,\n",
    "    'EQA_file': EQA_par_file,\n",
    "    'cumh5_file': cumh5_file,\n",
    "    'mask_file': mask_file,\n",
    "    'imdates': imdates,\n",
    "    'vel': vel,\n",
    "    'cum': cum,\n",
    "    'dates': dates,\n",
    "    'width': width,\n",
    "    'length': length,\n",
    "    'corner_lat': corner_lat,\n",
    "    'corner_lon': corner_lon,\n",
    "    'post_lat': post_lat,\n",
    "    'post_lon': post_lon,\n",
    "    'lat': lat,\n",
    "    'lon': lon\n",
    "        })\n",
    "\n",
    "# Create GeoDataFrame\n",
    "frames_gdf = gpd.GeoDataFrame(frames_data, columns=['frame', 'EQA_file', 'cumh5_file', 'mask_file', 'imdates', 'vel', 'cum', 'dates', 'width', 'length', 'corner_lat', 'corner_lon', 'post_lat', 'post_lon','lat', 'lon'])\n",
    "\n",
    "print(\"reshaping array\")\n",
    "# cum shape is (t, lat, lon)\n",
    "# we want to make an array of (time, pixels)\n",
    "# we want to reshape cum (202, 268, 327) into (202,(268*327))\n",
    "\n",
    "frames_gdf[\"cum_\"] = \"\"\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_data = row['cum']\n",
    "\n",
    "    # Check if 'cum' data is not empty\n",
    "    if not np.isnan(cum_data).all():\n",
    "        cum_shape = cum_data.shape\n",
    "        n_pix = cum_shape[1] * cum_shape[2]\n",
    "\n",
    "        cum_ = np.reshape(cum_data, (cum_shape[0], n_pix))\n",
    "        frames_gdf.at[index, 'cum_'] = cum_\n",
    "\n",
    "# we have cum of shape 202, 268, 327 and mask tif. We want to mask cum with mask tif \n",
    "# i.e. make pixels in cum nan where mask_tif = 0. NaNs are where no data e.g. outside of frame, low coh...\n",
    "\n",
    "print(\"masking array\")\n",
    "frames_gdf[\"cum_masked_2d\"] = \"\"\n",
    "frames_gdf[\"cum_masked_3d\"] = \"\"\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_data = row['cum_']\n",
    "    cum_data_3d = row['cum']\n",
    "    mask = row['mask_file']\n",
    "\n",
    "    with rasterio.open(mask[0]) as tif:\n",
    "    # Read the raster data\n",
    "        mask_tif = tif.read(1)\n",
    "\n",
    "        # Reshape the mask to 1D\n",
    "        mask_1d = mask_tif.flatten()\n",
    "\n",
    "        # Tile the first row along the rows to match the shape of asc_cum_\n",
    "        mask_2d = np.tile(mask_1d, (cum_data.shape[0],1))\n",
    "        \n",
    "        # Apply the mask to every element in the 3D array\n",
    "        cum_masked = cum_data * mask_2d\n",
    "        cum_masked_3d = cum_masked.reshape(cum_data_3d.shape)\n",
    "        \n",
    "        frames_gdf.at[index, 'cum_masked_2d'] = cum_masked\n",
    "        frames_gdf.at[index, 'cum_masked_3d'] = cum_masked_3d\n",
    "\n",
    "# Find columns (pixels) containing only zeros in the t-s and mask them\n",
    "print(\"removing pixels with only zeros\")\n",
    "frames_gdf[\"cum_no_nans_zeros\"] = \"\"\n",
    "frames_gdf[\"non_zero_ind\"] = \"\"\n",
    "frames_gdf[\"non_nan_ind\"] = \"\"\n",
    "\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    cum_masked = row['cum_masked_2d']\n",
    "\n",
    "    zero_pixels = np.all(cum_masked == 0, axis=0)\n",
    "    \n",
    "    # Create a new data array without columns (pixels) containing all zeros\n",
    "    cum_no_zeros = cum_masked[:, ~zero_pixels]\n",
    "    \n",
    "    # Print how many NaNs there are\n",
    "    nan_indices = np.argwhere(np.isnan(cum_no_zeros))\n",
    "\n",
    "    # find columns containing NaNs and zeroes\n",
    "    nan_pixels = np.any(np.isnan(cum_no_zeros), axis=0)\n",
    "\n",
    "    # create a new data array without nan columns (pixels)\n",
    "    cum_no_zeros_no_nans = cum_no_zeros[:, ~nan_pixels]\n",
    "\n",
    "    zero_ind = np.argwhere(zero_pixels).flatten()\n",
    "    non_zero_ind = np.argwhere(~zero_pixels).flatten()\n",
    "\n",
    "    nans = np.any(np.isnan(cum_masked), axis=0)\n",
    "    nan_ind = np.argwhere(nans).flatten()\n",
    "    non_nan_ind = np.argwhere(~nans).flatten()\n",
    "\n",
    "    frames_gdf.at[index, 'cum_no_nans_zeros'] = cum_no_zeros_no_nans\n",
    "    frames_gdf.at[index, 'non_zero_ind'] = non_zero_ind\n",
    "    frames_gdf.at[index, 'non_nan_ind'] = non_nan_ind\n",
    "\n",
    "# Apply spatial or temporal ICA\n",
    "frames_gdf[\"S_ft\"] = \"\"\n",
    "frames_gdf[\"restored_signals_2d\"] = \"\"\n",
    "frames_gdf[\"restored_signals_3d\"] = \"\"\n",
    "\n",
    "# attempt ICA\n",
    "ncomponents=n_components\n",
    "\n",
    "print(\"running ICA\")\n",
    "for index, row in frames_gdf.iterrows():\n",
    "    frame = row['frame']\n",
    "    data = row['cum_no_nans_zeros']\n",
    "    non_nan_ind = row['non_nan_ind']\n",
    "    non_zero_ind = row['non_zero_ind']\n",
    "    cum = row['cum']\n",
    "    \n",
    "    if approach == \"temporal\":\n",
    "        data_to_decompose = data.T  # Shape: pixels, time\n",
    "        if data_to_decompose.shape[0] > ncomponents and data.shape[1] > ncomponents:\n",
    "            # Perform ICA\n",
    "            # set up the transformer (ica). In MATLAB you do the whitening first then the transforming, here you do it in one.\n",
    "            ica = FastICA(n_components=ncomponents, whiten=\"unit-variance\")\n",
    "        \t\n",
    "            # fit the transformer to the data array\n",
    "            S_ft = ica.fit_transform(data_to_decompose) # fit model and recover signals\n",
    "            #S_t = ica.transform(data) # recover sources from x using unmixing matrix\n",
    "            ## S_ft and S_t results should be identical as ica.transform uses mixing matrix calculated by ica.fit_trcd ansform\n",
    "        \n",
    "            frames_gdf.at[index, 'S_ft'] = S_ft\n",
    "        \n",
    "            # Take each signal and restore with outer product\n",
    "            restored_signals_outer = []\n",
    "            reconstructed_data_test = []\n",
    "            three_d_list = []\n",
    "            for j in range(ncomponents):\n",
    "                S_j = np.copy(S_ft)\n",
    "                signal = S_j[:,j]\n",
    "                mixing = ica.mixing_[:,j]\n",
    "                restored_signal_j = np.outer(signal, mixing)\n",
    "                #reconstructed_data = ica.inverse_transform(S_ft) # test tp check method\n",
    "                \n",
    "                # Append the restored signal to the list\n",
    "                restored_signals_outer.append(restored_signal_j)\n",
    "                # reconstructed_data_test.append(reconstructed_data) test plot to check the same\n",
    "        \n",
    "                # Convert common_indices to 3D indices i.e. flat indices into a tuple for 3d array\n",
    "                # first find indices common to non nan and non zero for data population\n",
    "                common_indices = np.intersect1d(non_nan_ind, non_zero_ind)\n",
    "        \n",
    "                # create a shape the same size as original data with nans\n",
    "                cum_with_nans = np.full((cum.shape[0], cum.shape[1], cum.shape[2]), np.nan)\n",
    "        \n",
    "                # Convert common_indices to 3D indices i.e. flat indices into a tuple for 3d array\n",
    "                indices_3d = np.unravel_index(common_indices, cum.shape[1:])\n",
    "        \n",
    "                # Create a copy of cum_with_nans to work with for each restored_signal\n",
    "                cum_with_nans_copy = cum_with_nans.copy()\n",
    "\n",
    "                # need to make sure a different time series assigned to timestep ***\n",
    "                for m in range(restored_signal_j.shape[1]):\n",
    "                   # Assign values from the restored signal to non-NaN positions\n",
    "                    cum_with_nans_copy[m, indices_3d[0], indices_3d[1]] = restored_signal_j[:,m]\n",
    "                   \n",
    "                # for m in range(restored_signal_j.shape[1]):\n",
    "                #     # Assign values from the restored signal to non-NaN positions\n",
    "                #     cum_with_nans_copy[m, indices_3d[0], indices_3d[1]] = restored_signal_j[:,m]\n",
    "        \n",
    "                # reshape cum_with_nans into time x pixels\n",
    "                cum_with_nans_pix = cum_with_nans_copy.reshape(cum.shape[0], cum.shape[1] * cum.shape[2])\n",
    "        \n",
    "                #reshape cum_with_nans_pix into 3d to save signals\n",
    "                restored_signal_3d = cum_with_nans_pix.reshape(cum.shape[0], cum.shape[1], cum.shape[2])\n",
    "                three_d_list.append(restored_signal_3d)\n",
    "                \n",
    "            frames_gdf.at[index, 'restored_signals_2d'] = restored_signals_outer   \n",
    "            frames_gdf.at[index, 'restored_signals_3d'] = three_d_list\n",
    "        else:\n",
    "            print(\"Skipping ICA for frame {} because the number of features is not greater than {}.\".format(frame, ncomponents))\n",
    "    \n",
    "    elif approach == \"spatial\":\n",
    "        data_to_decompose = data # Shape: time, pixels\n",
    "\n",
    "        if data.shape[1] > ncomponents and data.shape[0] > ncomponents:\n",
    "        # Perform ICA\n",
    "            # set up the transformer (ica). In MATLAB you do the whitening first then the transforming, here you do it in one.\n",
    "            ica = FastICA(n_components=ncomponents, whiten=\"unit-variance\")\n",
    "        \t\n",
    "            # fit the transformer to the data array\n",
    "            S_ft = ica.fit_transform(data_to_decompose) # fit model and recover signals\n",
    "            #S_t = ica.transform(data) # recover sources from x using unmixing matrix\n",
    "            ## S_ft and S_t results should be identical as ica.transform uses mixing matrix calculated by ica.fit_trcd ansform\n",
    "        \n",
    "            frames_gdf.at[index, 'S_ft'] = S_ft\n",
    "        \n",
    "            # Take each signal and restore with outer product\n",
    "            restored_signals_outer = []\n",
    "            three_d_list = []\n",
    "            for j in range(ncomponents):\n",
    "                S_j = np.copy(S_ft)\n",
    "                signal = S_j[:,j]\n",
    "                mixing = ica.mixing_[:,j]\n",
    "                restored_signal_j = np.outer(signal, mixing)\n",
    "                \n",
    "                # Append the restored signal to the list\n",
    "                restored_signals_outer.append(restored_signal_j)\n",
    "        \n",
    "                # Convert common_indices to 3D indices i.e. flat indices into a tuple for 3d array\n",
    "                # first find indices common to non nan and non zero for data population\n",
    "                common_indices = np.intersect1d(non_nan_ind, non_zero_ind)\n",
    "        \n",
    "                # create a shape the same size as original data with nans\n",
    "                cum_with_nans = np.full((cum.shape[0], cum.shape[1], cum.shape[2]), np.nan)\n",
    "        \n",
    "                # Convert common_indices to 3D indices i.e. flat indices into a tuple for 3d array\n",
    "                indices_3d = np.unravel_index(common_indices, cum.shape[1:])\n",
    "        \n",
    "                # Create a copy of cum_with_nans to work with for each restored_signal\n",
    "                cum_with_nans_copy = cum_with_nans.copy()\n",
    "                \n",
    "                # need to make sure a different time series assigned to timestep\n",
    "                for m in range(restored_signal_j.shape[0]):\n",
    "                    # Assign values from the restored signal to non-NaN positions\n",
    "                    cum_with_nans_copy[m, indices_3d[0], indices_3d[1]] = restored_signal_j[m,:]\n",
    "        \n",
    "                # reshape cum_with_nans into time x pixels\n",
    "                cum_with_nans_pix = cum_with_nans_copy.reshape(cum.shape[0], cum.shape[1] * cum.shape[2])\n",
    "        \n",
    "                #reshape cum_with_nans_pix into 3d to save signals\n",
    "                restored_signal_3d = cum_with_nans_pix.reshape(cum.shape[0], cum.shape[1], cum.shape[2])\n",
    "                three_d_list.append(restored_signal_3d)\n",
    "                \n",
    "            frames_gdf.at[index, 'restored_signals_2d'] = restored_signals_outer   \n",
    "            frames_gdf.at[index, 'restored_signals_3d'] = three_d_list\n",
    "        else:\n",
    "            print(\"Skipping ICA for frame {} because the number of features is not greater than {}.\".format(frame, ncomponents))\n",
    "\n",
    "if approach == \"temporal\":\n",
    "    # find inelastic component, elastic component, plot things, save nc and tif\n",
    "    print(\"identifying elastic and inelastic components and save outputs\")\n",
    "    for index, row in frames_gdf.iterrows():\n",
    "        frame = row['frame']\n",
    "        dates = row['dates']\n",
    "        restored_signals_3d = row['restored_signals_3d']\n",
    "        restored_signals_2d = row['restored_signals_2d']\n",
    "        corner_lat = row['corner_lat']\n",
    "        corner_lon = row['corner_lon']\n",
    "        post_lon = row['post_lon']\n",
    "        post_lat = row['post_lat']\n",
    "        width = row['width']\n",
    "        height = row['length']\n",
    "        lat = row['lat']\n",
    "        lon = row['lon']\n",
    "        nc_data = row['imdates']\n",
    "        Sft = row['S_ft']\n",
    "    \n",
    "        # Only proceed if there are enough coherent pixels\n",
    "        if restored_signals_3d[0].shape[1] >= 20:\n",
    "            # List to store R-squared values for each trend\n",
    "            mean_gradient = []\n",
    "            median_r_squared = []\n",
    "            mean_second_derivative = []\n",
    "            median_mae = []\n",
    "    \n",
    "            # Convert dates to numerical values\n",
    "            num_dates = date2num(dates)\n",
    "    \n",
    "            for m, signal_0 in enumerate(restored_signals_2d):\n",
    "                r_squared_values = []\n",
    "                gradient = []\n",
    "                second_deriv = []\n",
    "                maes = []\n",
    "\n",
    "                # transpose signal for linear regression etc.\n",
    "                signal = signal_0.T\n",
    "    \n",
    "                # Loop through each trend at each pixel\n",
    "                for i in range(signal.shape[1]):\n",
    "                    # Perform linear regression and calculate R-squared\n",
    "                    slope, _, r_value, _, _ = linregress(num_dates, signal[:, i]) \n",
    "    \n",
    "                    # Store the gradient at each pixel\n",
    "                    gradient.append(slope)\n",
    "    \n",
    "                    # Append R-squared value to the list\n",
    "                    r_squared_values.append(r_value ** 2)\n",
    "    \n",
    "                    # Calculate the second derivative using numpy.gradient at each pixel\n",
    "                    first_derivative = np.gradient(signal[:, i], num_dates)\n",
    "                    second_derivative = np.gradient(first_derivative, num_dates)\n",
    "                    second_derivative_mean = np.mean(second_derivative)\n",
    "                    second_deriv.append(second_derivative_mean)\n",
    "    \n",
    "                    # choose model 1 (annual + L) and then calculate fit useing MAE\n",
    "                    yvalues = calc_model(signal[:, i], num_dates, 3)\n",
    "                    mae_annual = mean_absolute_error(signal[:, i], yvalues)\n",
    "                    maes.append(mae_annual)\n",
    "                        \n",
    "                # take mean of gradients\n",
    "                mean_gradient.append(np.mean(gradient))\n",
    "    \n",
    "                # take median of r_squared per IC\n",
    "                median_r_squared.append(np.median(r_squared_values))\n",
    "    \n",
    "                # find median mae of fit of model and signal\n",
    "                median_mae.append(np.median(maes))\n",
    "    \n",
    "                # take mean of second derivative per IC\n",
    "                mean_second_derivative.append(np.mean(second_deriv))\n",
    "    \n",
    "                # plot components in space and time with parameters\n",
    "                fig = f\"{frame}_component_{m}.png\"\n",
    "                fig_directory = os.path.join(plot_dir, approach, f\"{n_components}_components\", frame)\n",
    "    \n",
    "                # Check if the directory exists, if not, create it\n",
    "                if not os.path.exists(fig_directory):\n",
    "                        os.makedirs(fig_directory)\n",
    "                \n",
    "                # plot components in space and time with parameters\n",
    "                plt.figure(figsize=(16,5))\n",
    "                plt.subplot2grid((1, 3), (0, 0))\n",
    "                plt.plot(dates,signal)\n",
    "    \n",
    "                # Adding text to the top right of the plot\n",
    "                plt.text(0.95, 0.95, f\"Gradient: {np.mean(gradient):.4f}\\nR Squared: {np.median(r_squared_values):.4f}\\nMAE: {np.median(maes):.4f}\\nSecond Derivative: {np.mean(second_deriv):.4f}\",\n",
    "                     horizontalalignment='right',\n",
    "                     verticalalignment='top',\n",
    "                     transform=plt.gca().transAxes,\n",
    "                     bbox=dict(facecolor='white', alpha=0.5))\n",
    "                \n",
    "                plt.subplot2grid((1, 3), (0, 1))\n",
    "                plt.imshow(restored_signals_3d[m][-1,:,:], cmap='viridis', interpolation='none', extent=[np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)])\n",
    "                cbar = plt.colorbar(label='mm/yr', shrink=0.6)\n",
    "                plt.text(0.95, 0.95, \"Reconstructed time series\",\n",
    "                     horizontalalignment='right',\n",
    "                     verticalalignment='top',\n",
    "                     transform=plt.gca().transAxes,\n",
    "                     bbox=dict(facecolor='white', alpha=0.5))\n",
    "                \n",
    "                plt.subplot2grid((1, 3), (0, 2))\n",
    "                plt.plot(dates,ica.mixing_[:,m], label = 'Component time series {}'.format(m))\n",
    "                plt.legend()\n",
    "                print('Saving component figure at {}/{}'.format(fig_directory, fig))\n",
    "                plt.savefig(os.path.join(fig_directory, fig))\n",
    "                plt.close()\n",
    "    \n",
    "    #-----------------------------#\n",
    "            # If mean gradient is negative, find the index of the trend with the maximum R-squared value\n",
    "            negative_indices = [i for i, val in enumerate(mean_gradient) if val < 0]\n",
    "            if negative_indices:\n",
    "                median_r_squared_negative_gradients = [median_r_squared[i] for i in negative_indices]\n",
    "                max_r_squared_negative_grad = np.max(median_r_squared_negative_gradients)\n",
    "                max_index_in_median_r_squared = median_r_squared.index(max_r_squared_negative_grad)\n",
    "                \n",
    "        \t    # Choose the signal\n",
    "                inelastic_signal = restored_signals_3d[max_index_in_median_r_squared]\n",
    "                inelastic_signal_subsiding = restored_signals_2d[max_index_in_median_r_squared]\n",
    "                inelastic_S_ft = ica.mixing_[:,max_index_in_median_r_squared]\n",
    "                \n",
    "                print('mean_gradient',mean_gradient)\n",
    "                print('median_r_squared',median_r_squared)\n",
    "                print('median_mae',median_mae)\n",
    "                print('mean_second_derivative',mean_second_derivative)\n",
    "                \n",
    "        \t    # Save as NetCDF\n",
    "                output_dir = os.path.join(out_dir, approach, f\"{ncomponents}_comp\", frame)\n",
    "    \n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "    \n",
    "                # Remove all files in the directory\n",
    "                for filename in os.listdir(output_dir):\n",
    "                    file_path = os.path.join(output_dir, filename)\n",
    "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                        os.unlink(file_path)  # Remove the file or link\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)  # Remove the directory and its contents\n",
    "                            \n",
    "                # cp inelastic png to outputs\n",
    "                #shutil.copy(os.path.join(fig_directory, f\"{frame}_component_{max_index_in_median_r_squared}.png\"),output_dir)\n",
    "    \n",
    "                print('Saving inelastic component nc and tif at {}'.format(output_dir))\n",
    "    \n",
    "                output_nc_path = os.path.join(output_dir, f\"{frame}_{ncomponents}_components.nc\")\n",
    "                with nc.Dataset(output_nc_path, 'w') as file:\n",
    "                    # Create dimensions\n",
    "                    file.createDimension('dates', inelastic_signal.shape[0])\n",
    "                    file.createDimension('latitude', inelastic_signal.shape[1])\n",
    "                    file.createDimension('longitude', inelastic_signal.shape[2])\n",
    "                    \n",
    "                    # Create variables\n",
    "                    time_var = file.createVariable('dates', np.int32, ('dates',))\n",
    "                    lat_var = file.createVariable('latitude', 'f4', ('latitude',))\n",
    "                    lon_var = file.createVariable('longitude', 'f4', ('longitude',))\n",
    "                    data_var = file.createVariable('inelastic_reconstructed_signals', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                    component_var = file.createVariable('inelastic_component', 'f4', ('dates'))\n",
    "                    mae_var = file.createVariable('inelastic_MAE', 'f4')\n",
    "                    second_derivative_var = file.createVariable('inelastic_second_derivative', 'f4')\n",
    "                    \n",
    "                    # Add data to variables\n",
    "                    time_var[:] = nc_data\n",
    "                    lat_var[:] = lat\n",
    "                    lon_var[:] = lon\n",
    "                    data_var[:] = inelastic_signal\n",
    "                    component_var[:] = inelastic_S_ft\n",
    "                    mae_var[:] = median_mae[max_index_in_median_r_squared]\n",
    "                    second_derivative_var[:] = mean_second_derivative[max_index_in_median_r_squared]\n",
    "    \n",
    "                    # Create variables for each component\n",
    "                    data_vars = []\n",
    "                    component_vars = []\n",
    "                    mae_vars = []\n",
    "                    second_derivative_vars = []\n",
    "                    \n",
    "                    for comp_index in range(ncomponents - 1):\n",
    "                        data_var = file.createVariable(f'reconstructed_signals_{comp_index}', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                        component_var = file.createVariable(f'component_{comp_index}', 'f4', ('dates'))\n",
    "                        mae_var = file.createVariable(f'mae_{comp_index}', 'f4')\n",
    "                        second_derivative_var = file.createVariable(f'second_derivative_{comp_index}', 'f4')\n",
    "                        \n",
    "                        data_vars.append(data_var)\n",
    "                        component_vars.append(component_var)\n",
    "                        mae_vars.append(mae_var)\n",
    "                        second_derivative_vars.append(second_derivative_var)\n",
    "    \n",
    "                    # Add data, mae, and second derivative to variables\n",
    "                    comp_index=0\n",
    "                    for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                        if i != max_index_in_median_r_squared:\n",
    "                            data_vars[comp_index][:] = signal_3d\n",
    "                            component_vars[comp_index][:] = ica.mixing_[:,i] \n",
    "                            mae_vars[comp_index][:] = median_mae[i]\n",
    "                            second_derivative_vars[comp_index][:] = mean_second_derivative[i]\n",
    "                            comp_index += 1\n",
    "    \n",
    "                # cp elastic png to outputs\n",
    "                for i in range(len(restored_signals_3d)):\n",
    "                    shutil.copy(os.path.join(fig_directory, f\"{frame}_component_{i}.png\"), output_dir)\n",
    "    \n",
    "                print(\"Saving other component tifs at {}\".format(output_dir))\n",
    "    \n",
    "                # Save as GeoTIFF\n",
    "                output_tif_path = os.path.join(output_dir, f\"{frame}_inelastic_component_{max_index_in_median_r_squared}.tif\")\n",
    "    \n",
    "                # Create a transformation for the GeoTIFF\n",
    "                post_lat_pos = post_lat * (-1)\n",
    "                transform = from_origin(corner_lon, corner_lat, post_lon, post_lat_pos)\n",
    "    \n",
    "                with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                                   dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                    # Write the data to the GeoTIFF\n",
    "                    dst.write(inelastic_signal[-1, :, :], 1)\n",
    "                               \n",
    "                for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                    if i != max_index_in_median_r_squared:\n",
    "                        output_tif_path = os.path.join(output_dir, f\"{frame}_component_{i}.tif\")\n",
    "                        with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                                           dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                            # Write the data to the GeoTIFF\n",
    "                            dst.write(signal_3d[-1, :, :], 1)\n",
    "\n",
    "elif approach == \"spatial\":\n",
    "    # find inelastic component, elastic component, plot things, save nc and tif\n",
    "    print(\"identifying elastic and inelastic components and save outputs\")\n",
    "    for index, row in frames_gdf.iterrows():\n",
    "        frame = row['frame']\n",
    "        dates = row['dates']\n",
    "        restored_signals_3d = row['restored_signals_3d']\n",
    "        restored_signals_2d = row['restored_signals_2d']\n",
    "        corner_lat = row['corner_lat']\n",
    "        corner_lon = row['corner_lon']\n",
    "        post_lon = row['post_lon']\n",
    "        post_lat = row['post_lat']\n",
    "        width = row['width']\n",
    "        height = row['length']\n",
    "        lat = row['lat']\n",
    "        lon = row['lon']\n",
    "        nc_data = row['imdates']\n",
    "        Sft = row['S_ft']\n",
    "    \n",
    "        # Only proceed if there are enough coherent pixels\n",
    "        if restored_signals_2d[0].shape[1] >= 20:\n",
    "            # List to store R-squared values for each trend\n",
    "            mean_gradient = []\n",
    "            median_r_squared = []\n",
    "            mean_second_derivative = []\n",
    "            median_mae = []\n",
    "    \n",
    "            # Convert dates to numerical values\n",
    "            num_dates = date2num(dates)\n",
    "    \n",
    "            for m, signal in enumerate(restored_signals_2d):\n",
    "                r_squared_values = []\n",
    "                gradient = []\n",
    "                second_deriv = []\n",
    "                maes = []\n",
    "    \n",
    "                # Loop through each trend at each pixel\n",
    "                for i in range(signal.shape[1]):\n",
    "                    # Perform linear regression and calculate R-squared\n",
    "                    slope, _, r_value, _, _ = linregress(num_dates, signal[:, i])\n",
    "                    # Store the gradient at each pixel\n",
    "                    gradient.append(slope)\n",
    "    \n",
    "                    # Append R-squared value to the list\n",
    "                    r_squared_values.append(r_value ** 2)\n",
    "    \n",
    "                    # Calculate the second derivative using numpy.gradient at each pixel\n",
    "                    first_derivative = np.gradient(signal[:, i], num_dates)\n",
    "                    second_derivative = np.gradient(first_derivative, num_dates)\n",
    "                    second_derivative_mean = np.mean(second_derivative)\n",
    "                    second_deriv.append(second_derivative_mean)\n",
    "    \n",
    "                    # choose model 1 (annual + L) and then calculate fit useing MAE\n",
    "                    yvalues = calc_model(signal[:, i], num_dates, 3)\n",
    "                    mae_annual = mean_absolute_error(signal[:, i], yvalues)\n",
    "                    maes.append(mae_annual)\n",
    "                        \n",
    "                # take mean of gradients\n",
    "                mean_gradient.append(np.mean(gradient))\n",
    "                plt.hist(gradient)\n",
    "    \n",
    "                # take median of r_squared per IC\n",
    "                median_r_squared.append(np.median(r_squared_values))\n",
    "    \n",
    "                # find median mae of fit of model and signal\n",
    "                median_mae.append(np.median(maes))\n",
    "    \n",
    "                # take mean of second derivative per IC\n",
    "                mean_second_derivative.append(np.mean(second_deriv))\n",
    "    \n",
    "                # plot components in space and time with parameters\n",
    "                fig = f\"{frame}_component_{m}.png\"\n",
    "                fig_directory = os.path.join(plot_dir, approach, f\"{n_components}_components\", frame)\n",
    "    \n",
    "                # Check if the directory exists, if not, create it\n",
    "                if not os.path.exists(fig_directory):\n",
    "                        os.makedirs(fig_directory)\n",
    "                \n",
    "                # plot components in space and time with parameters\n",
    "                plt.figure(figsize=(16,5))\n",
    "                plt.subplot2grid((1, 3), (0, 0))\n",
    "                plt.plot(dates,signal)\n",
    "    \n",
    "                # Adding text to the top right of the plot\n",
    "                plt.text(0.95, 0.95, f\"Gradient: {np.mean(gradient):.4f}\\nR Squared: {np.median(r_squared_values):.4f}\\nMAE: {np.median(maes):.4f}\\nSecond Derivative: {np.mean(second_deriv):.4f}\",\n",
    "                     horizontalalignment='right',\n",
    "                     verticalalignment='top',\n",
    "                     transform=plt.gca().transAxes,\n",
    "                     bbox=dict(facecolor='white', alpha=0.5))\n",
    "                \n",
    "                plt.subplot2grid((1, 3), (0, 1))\n",
    "                plt.imshow(restored_signals_3d[m][-1,:,:], cmap='viridis', interpolation='none', extent=[np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)])\n",
    "                cbar = plt.colorbar(label='mm/yr', shrink=0.6)\n",
    "                plt.text(0.95, 0.95, \"Reconstructed time series\",\n",
    "                     horizontalalignment='right',\n",
    "                     verticalalignment='top',\n",
    "                     transform=plt.gca().transAxes,\n",
    "                     bbox=dict(facecolor='white', alpha=0.5))\n",
    "                \n",
    "                plt.subplot2grid((1, 3), (0, 2))\n",
    "                plt.plot(dates,S_ft[:,m], label = 'Component time series {}'.format(m))\n",
    "                plt.legend()\n",
    "                print('Saving component figure at {}/{}'.format(fig_directory, fig))\n",
    "                plt.savefig(os.path.join(fig_directory, fig))\n",
    "                plt.close()\n",
    "    \n",
    "    #-----------------------------#\n",
    "            # If mean gradient is negative, find the index of the trend with the maximum R-squared value\n",
    "            negative_indices = [i for i, val in enumerate(mean_gradient) if val < 0]\n",
    "            if negative_indices:\n",
    "                median_r_squared_negative_gradients = [median_r_squared[i] for i in negative_indices]\n",
    "                max_r_squared_negative_grad = np.max(median_r_squared_negative_gradients)\n",
    "                max_index_in_median_r_squared = median_r_squared.index(max_r_squared_negative_grad)\n",
    "                \n",
    "        \t    # Choose the signal\n",
    "                inelastic_signal = restored_signals_3d[max_index_in_median_r_squared]\n",
    "                inelastic_signal_subsiding = restored_signals_2d[max_index_in_median_r_squared]\n",
    "                inelastic_S_ft = S_ft[:,max_index_in_median_r_squared]\n",
    "                \n",
    "                print('mean_gradient',mean_gradient)\n",
    "                print('median_r_squared',median_r_squared)\n",
    "                print('median_mae',median_mae)\n",
    "                print('mean_second_derivative',mean_second_derivative)\n",
    "                \n",
    "        \t    # Save as NetCDF\n",
    "                output_dir = os.path.join(out_dir, approach, f\"{ncomponents}_comp\", frame)\n",
    "    \n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "    \n",
    "                # Remove all files in the directory\n",
    "                for filename in os.listdir(output_dir):\n",
    "                    file_path = os.path.join(output_dir, filename)\n",
    "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                        os.unlink(file_path)  # Remove the file or link\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)  # Remove the directory and its contents\n",
    "                            \n",
    "                # cp inelastic png to outputs\n",
    "                #shutil.copy(os.path.join(fig_directory, f\"{frame}_component_{max_index_in_median_r_squared}.png\"),output_dir)\n",
    "    \n",
    "                print('Saving inelastic component nc and tif at {}'.format(output_dir))\n",
    "    \n",
    "                output_nc_path = os.path.join(output_dir, f\"{frame}_{ncomponents}_components.nc\")\n",
    "                with nc.Dataset(output_nc_path, 'w') as file:\n",
    "                    # Create dimensions\n",
    "                    file.createDimension('dates', inelastic_signal.shape[0])\n",
    "                    file.createDimension('latitude', inelastic_signal.shape[1])\n",
    "                    file.createDimension('longitude', inelastic_signal.shape[2])\n",
    "                    \n",
    "                    # Create variables\n",
    "                    time_var = file.createVariable('dates', np.int32, ('dates',))\n",
    "                    lat_var = file.createVariable('latitude', 'f4', ('latitude',))\n",
    "                    lon_var = file.createVariable('longitude', 'f4', ('longitude',))\n",
    "                    data_var = file.createVariable('inelastic_reconstructed_signals', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                    component_var = file.createVariable('inelastic_component', 'f4', ('dates'))\n",
    "                    mae_var = file.createVariable('inelastic_MAE', 'f4')\n",
    "                    second_derivative_var = file.createVariable('inelastic_second_derivative', 'f4')\n",
    "                    \n",
    "                    # Add data to variables\n",
    "                    time_var[:] = nc_data\n",
    "                    lat_var[:] = lat\n",
    "                    lon_var[:] = lon\n",
    "                    data_var[:] = inelastic_signal\n",
    "                    component_var[:] = inelastic_S_ft\n",
    "                    mae_var[:] = median_mae[max_index_in_median_r_squared]\n",
    "                    second_derivative_var[:] = mean_second_derivative[max_index_in_median_r_squared]\n",
    "    \n",
    "                    # Create variables for each component\n",
    "                    data_vars = []\n",
    "                    component_vars = []\n",
    "                    mae_vars = []\n",
    "                    second_derivative_vars = []\n",
    "                    \n",
    "                    for comp_index in range(ncomponents - 1):\n",
    "                        data_var = file.createVariable(f'reconstructed_signals_{comp_index}', 'f4', ('dates', 'latitude', 'longitude'))\n",
    "                        component_var = file.createVariable(f'component_{comp_index}', 'f4', ('dates'))\n",
    "                        mae_var = file.createVariable(f'mae_{comp_index}', 'f4')\n",
    "                        second_derivative_var = file.createVariable(f'second_derivative_{comp_index}', 'f4')\n",
    "                        \n",
    "                        data_vars.append(data_var)\n",
    "                        component_vars.append(component_var)\n",
    "                        mae_vars.append(mae_var)\n",
    "                        second_derivative_vars.append(second_derivative_var)\n",
    "    \n",
    "                    # Add data, mae, and second derivative to variables\n",
    "                    comp_index=0\n",
    "                    for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                        if i != max_index_in_median_r_squared:\n",
    "                            data_vars[comp_index][:] = signal_3d\n",
    "                            component_vars[comp_index][:] = S_ft[:,i] \n",
    "                            mae_vars[comp_index][:] = median_mae[i]\n",
    "                            second_derivative_vars[comp_index][:] = mean_second_derivative[i]\n",
    "                            comp_index += 1\n",
    "    \n",
    "                # cp elastic png to outputss\n",
    "                for i in range(len(restored_signals_3d)):\n",
    "                    shutil.copy(os.path.join(fig_directory, f\"{frame}_component_{i}.png\"), output_dir)\n",
    "    \n",
    "                print(\"Saving other component tifs at {}\".format(output_dir))\n",
    "    \n",
    "                # Save as GeoTIFF\n",
    "                output_tif_path = os.path.join(output_dir, f\"{frame}_inelastic_component_{max_index_in_median_r_squared}.tif\")\n",
    "    \n",
    "                # Create a transformation for the GeoTIFF\n",
    "                post_lat_pos = post_lat * (-1)\n",
    "                transform = from_origin(corner_lon, corner_lat, post_lon, post_lat_pos)\n",
    "    \n",
    "                with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                                   dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                    # Write the data to the GeoTIFF\n",
    "                    dst.write(inelastic_signal[-1, :, :], 1)\n",
    "                               \n",
    "                for i, signal_3d in enumerate(restored_signals_3d):\n",
    "                    if i != max_index_in_median_r_squared:\n",
    "                        output_tif_path = os.path.join(output_dir, f\"{frame}_component_{i}.tif\")\n",
    "                        with rasterio.open(output_tif_path, 'w', driver='GTiff', height=height, width=width, count=1,\n",
    "                                           dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "                            # Write the data to the GeoTIFF\n",
    "                            dst.write(signal_3d[-1, :, :], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb8fe3-2b5d-43f9-93e1-3dff5cb9c707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambalics",
   "language": "python",
   "name": "mambalics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
